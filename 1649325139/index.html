<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Scrapy 整理 | </title><meta name="keywords" content="scrapy"><meta name="author" content="Legacy"><meta name="copyright" content="Legacy"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Scrapy 框架​    Scrapy 是一个为了爬取网站数据，提取结构性数据而编写的应用框架。其最初是为了页面抓取 (更确切来说是网络抓取) 所设计的， 也可以应用在获取 API 所返回的数据 (例如 Amazon Associates Web Services ) 或者通用的网络爬虫。Scrapy 用途广泛，可以用于数据挖掘、监测和自动化测试，是纯 python 实现的，基于 Twisted">
<meta property="og:type" content="article">
<meta property="og:title" content="Scrapy 整理">
<meta property="og:url" content="http://example.com/1649325139/index.html">
<meta property="og:site_name">
<meta property="og:description" content="Scrapy 框架​    Scrapy 是一个为了爬取网站数据，提取结构性数据而编写的应用框架。其最初是为了页面抓取 (更确切来说是网络抓取) 所设计的， 也可以应用在获取 API 所返回的数据 (例如 Amazon Associates Web Services ) 或者通用的网络爬虫。Scrapy 用途广泛，可以用于数据挖掘、监测和自动化测试，是纯 python 实现的，基于 Twisted">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://wei-foun.github.io/img/cover25.jpg">
<meta property="article:published_time" content="2020-11-17T13:42:45.000Z">
<meta property="article:modified_time" content="2025-04-01T17:58:07.105Z">
<meta property="article:author" content="Legacy">
<meta property="article:tag" content="scrapy">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wei-foun.github.io/img/cover25.jpg"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="canonical" href="http://example.com/1649325139/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Scrapy 整理',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-04-02 01:58:07'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">49</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">31</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">46</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 文章</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 爱好收集</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://wei-foun.github.io/img/cover25.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/"></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 文章</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 爱好收集</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Scrapy 整理</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-11-17T13:42:45.000Z" title="发表于 2020-11-17 21:42:45">2020-11-17</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-04-01T17:58:07.105Z" title="更新于 2025-04-02 01:58:07">2025-04-02</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%88%AC%E8%99%AB/scrapy-%E6%95%B4%E7%90%86/">scrapy 整理</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Scrapy 整理"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h3 id="Scrapy-框架"><a href="#Scrapy-框架" class="headerlink" title="Scrapy 框架"></a>Scrapy 框架</h3><p>​    Scrapy 是一个为了爬取网站数据，提取结构性数据而编写的应用框架。其最初是为了页面抓取 (更确切来说是网络抓取) 所设计的， 也可以应用在获取 API 所返回的数据 (例如 Amazon Associates Web Services ) 或者通用的网络爬虫。Scrapy 用途广泛，可以用于数据挖掘、监测和自动化测试，是纯 python 实现的，<strong>基于 Twisted 的异步处理框架</strong></p>
<p>​    安装：可以直接使用 pip 去安装   <code>pip install scrapy</code></p>
<h4 id="创建一个-scrapy-项目"><a href="#创建一个-scrapy-项目" class="headerlink" title="创建一个 scrapy 项目"></a>创建一个 scrapy 项目</h4><p>​    安装完成后，可以直接使用命令  <code>scrapy startproject 项目名称</code> 来创建一个项目</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">项目名称</span><br><span class="line">   - 项目名称</span><br><span class="line">      - spiders              # 所有该项目的爬虫目录</span><br><span class="line">         - ...               # 爬虫程序文件</span><br><span class="line">      - items.py             # 设置数据存储模板，用来提取并结构化数据</span><br><span class="line">      - pipelines.py         # 数据行为处理，比如对数据进行持久化</span><br><span class="line">      - middlewares.py       # 中间件处理</span><br><span class="line">      - settings.py          # 项目爬虫所用的配置文件，例如递归层数，并发数，延迟下载等</span><br><span class="line">   - scrapy.cfg              # 项目的配置信息</span><br></pre></td></tr></table></figure>
<h4 id="创建一个爬虫"><a href="#创建一个爬虫" class="headerlink" title="创建一个爬虫"></a>创建一个爬虫</h4><p>​    需要进入到项目目录下，使用命令   <code>scrapy genspider 名称 [域名]</code>， 域名 这个参数是可选项，如果使用这个参数，那么不要写前面的 www，比如  <code>scrapy genspider baidu baidu.com</code> ，完成后则会在 spiders 文件夹下就会出现对应的爬虫文件</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MovieSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;movie&#x27;</span></span><br><span class="line">    <span class="comment"># allowed_domains = [&#x27;xxx.com&#x27;]</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;https://www.4567tv.tv/index.php/vod/show/area/%E7%BE%8E%E5%9B%BD/id/1/year/2019.html&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        li_list = response.xpath(<span class="string">&quot;//ul[@class=&#x27;stui-vodlist clearfix&#x27;]/li&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(li_list)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>​    spider 会根据命令中的名称，生成一个 py 文件，并创建一个类，同时继承 spider.Spider，其中 <strong>name 属性是必不可少的且是唯一的</strong>，用来区分 spider 目录下各个不同的爬虫文件</p>
<p>​    allowed_domains 属性则是可选属性，包含允许爬取的域名列表，当   <code>OffsiteMiddleware</code> 启用时， 域名不在列表中的 url 不会被跟进爬取</p>
<p>​    start_urls 则是作为 spider 程序启动的初始爬取 url；同时会默认创建一个方法 parse，并且包含一个参数 response 表示下载器得到的响应结果，parse 方法用来对 response 进行处理或是继续跟进新的 url</p>
<h4 id="启动一个爬虫程序"><a href="#启动一个爬虫程序" class="headerlink" title="启动一个爬虫程序"></a>启动一个爬虫程序</h4><p>​    命令行输入  <code>scrapy crawl 爬虫文件名</code>，但是这样所有的状态信息返回都是在命令行工具中，并不是十分方便查看，另外如要爬取的内容要保存的到指定类型的文件，在该命令中可以使用 <code>-o</code>  参数指定，例如 json，csv，xml 等等</p>
<p>​    为了可以在 pycharm 中进行调试，可以去创建一个 main.py 脚本文件来作为爬虫程序的启动文件</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys, os</span><br><span class="line"><span class="keyword">from</span> scrapy.cmdline <span class="keyword">import</span> execute</span><br><span class="line"></span><br><span class="line">sys.path.append(os.path.abspath(os.path.dirname(__file__)))</span><br><span class="line"><span class="comment"># 将当前文件目录加入到 python 的 sys 搜索目录中</span></span><br><span class="line"></span><br><span class="line">execute([<span class="string">&quot;scrapy&quot;</span>,<span class="string">&quot;crawl&quot;</span>, <span class="string">&quot;baidu&quot;</span>])</span><br></pre></td></tr></table></figure>
<h4 id="架构组件"><a href="#架构组件" class="headerlink" title="架构组件"></a>架构组件</h4><p>​    <img src="https://wei-foun.github.io/img/scrapy-%E6%9E%B6%E6%9E%84.jpg" alt="img"> </p>
<p>​    <strong>scrapy engine：</strong>整个框架的中心处理引擎，用来处理整个系统的数据流，触发事务操作</p>
<p>​    <strong>scheduler：</strong>调度器负责从引擎中接收 request 并将他们加入到队列，然后再由引擎请求时返回</p>
<p>​    <strong>downloader：</strong>下载器的任务就是从 scheduler 中发送的网址在网络上进行下载，并将响应的内容进行返回交给 spider，而且 <strong>scrapy 的下载器是基于 twisted 异步模块实现</strong>，由此提升下载效率</p>
<p>​    <strong>spider：</strong>用于分析响应的 response 并提取 item，或是将要跟进的 url 再交给 scheduler 去跟进下载</p>
<p>​    <strong>item pipeline：</strong>负责处理被 spider 提取的 item，比如做数据持久化，验证有效性，或是清除不必要的信息</p>
<p>​    <strong>downloader middlewares：</strong>下载中间件是引擎与下载器之间特定钩子（specific hook），用于全局修改来自引擎给出的 request 和返回给引擎的 response</p>
<p>​    <strong>spider middlewares：</strong>则是介于引擎与 spider 之间的特定钩子，用于修改处理得到的 response 和需要跟进 request 以及提取的 item</p>
<h4 id="一个-spider-数据流执行过程"><a href="#一个-spider-数据流执行过程" class="headerlink" title="一个 spider 数据流执行过程"></a>一个 spider 数据流执行过程</h4><p>​    1）引擎会根据网站，找到对应的 spider 并向这个 spider 请求第一要爬取的 url</p>
<p>​    2）引擎从 spider 中拿到 url 后，送到调度器中</p>
<p>​    3）引擎向调度器请求爬取的 url，并将其封装成一个 request，通过下载中间件后，交给下载器进行网页下载</p>
<p>​    4）下载器完成对应 url 的网站页面下载后，得到响应的 response，再经由下载中间件返回给引擎</p>
<p>​    5）引擎接收到下载器返回的响应，将 response 通过 spider 中间件交给 spider 进行处理</p>
<p>​    6）spider 处理后，从 response 中提取需要得到 item 和需要跟进的 url 继续返回给引擎</p>
<p>​    7）引擎将 spider 给的 item 给到 item pipeline 做后续处理，url 则是继续之前的步骤给调度器完成之后的爬取和提取，直到最后整个引擎关闭</p>
<h3 id="提取方式"><a href="#提取方式" class="headerlink" title="提取方式"></a>提取方式</h3><p>​    scrapy 的 response 即支持 xpath 提取 item，也可以使用 css 选择器去提取，<strong>另外也支持通过 re 方法使用正则进行提取，前提是 re 方法不能直接在返回的 response 上使用，会排除没有 re 方法的错误，如果使用 re 可以先使用 xpath 选中全文，在通过 re 正则进行解析提取</strong></p>
<h4 id="Xpath"><a href="#Xpath" class="headerlink" title="Xpath"></a>Xpath</h4><table>
<thead>
<tr>
<th>语法</th>
<th>解释</th>
</tr>
</thead>
<tbody><tr>
<td>article</td>
<td>选取所有article元素的所有子节点</td>
</tr>
<tr>
<td>/article</td>
<td>选取根元素article</td>
</tr>
<tr>
<td>article/a</td>
<td>选取所有article下的是a元素的<strong>子元素</strong></td>
</tr>
<tr>
<td>//div</td>
<td>选取所有div子元素（ // 表示不论是在文档任何位置）</td>
</tr>
<tr>
<td>article//div</td>
<td>选取所有article元素下只要是div元素的，无论是子代还是后代，无论在article下的任何位置 a</td>
</tr>
<tr>
<td>//@class</td>
<td>选取所有class属性的元素</td>
</tr>
<tr>
<td>/article/div[1]</td>
<td>选取article子元素下的第一个div元素</td>
</tr>
<tr>
<td>/article/div[last()]</td>
<td>选取article子元素下的最后一个div元素</td>
</tr>
<tr>
<td>/article/div[last()-1]</td>
<td>选取article子元素下的倒数第二个div元素</td>
</tr>
<tr>
<td>//div[@lang]</td>
<td>选取所有含有属性lang的div元素</td>
</tr>
<tr>
<td>//div[@lang=’eng’]</td>
<td>选取所有lang属性，且值是eng的div元素</td>
</tr>
<tr>
<td>/div/*</td>
<td>选取所有属于div元素的子节点</td>
</tr>
<tr>
<td>//*</td>
<td>选取所有元素</td>
</tr>
<tr>
<td>/div/a | /div/p</td>
<td>选取所有属于div下子元素的a和p元素</td>
</tr>
<tr>
<td>//span | //ul</td>
<td>选取所有span和ul的元素</td>
</tr>
</tbody></table>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># 属性定位</span><br><span class="line">//div[@class=&quot;song&quot;]     找到class是song的div元素</span><br><span class="line"></span><br><span class="line"># 层级和索引定位</span><br><span class="line">//div[@class=&quot;song&quot;]/ul/li[2]/a     找到class是song的div元素的直系子标签ul下第二个li标签下的a元素，xpath中索引的开始为是从1开始</span><br><span class="line"></span><br><span class="line"># 逻辑运算</span><br><span class="line">//a[@href=&quot;&quot; and @clss=&quot;song&quot;]     找到href属性为空，且class属性是song的a元素</span><br><span class="line"></span><br><span class="line"># 模糊匹配</span><br><span class="line">//div[contains(@class, &quot;ng&quot;)]     找到class属性值中包含 ng 的所有div元素</span><br><span class="line">//div[starts_with(@class, &quot;so&quot;)]     找到class属性值是 so 开头的所有div元素</span><br><span class="line"></span><br><span class="line"># 提取标签的文本内容</span><br><span class="line">//div[@class=&quot;song&quot;]/p[1]/text()     提取class是song的div下第一个p标签的所有文本内容</span><br><span class="line"></span><br><span class="line"># 提取属性的值</span><br><span class="line">//div[@class=&quot;song&quot;]/a/@herf     提取class是song的div下a标签中的href属性的值</span><br></pre></td></tr></table></figure>
<h4 id="CSS-选择器"><a href="#CSS-选择器" class="headerlink" title="CSS 选择器"></a>CSS 选择器</h4><table>
<thead>
<tr>
<th>语法</th>
<th>解释</th>
</tr>
</thead>
<tbody><tr>
<td>*</td>
<td>选取所有节点</td>
</tr>
<tr>
<td>#container</td>
<td>选取id为container的节点</td>
</tr>
<tr>
<td>.container</td>
<td>选取class包含的container节点</td>
</tr>
<tr>
<td>li a</td>
<td>选取所有li下的a节点（这包含了li的子代以及孙代）</td>
</tr>
<tr>
<td>ul+p</td>
<td>选取ul标签的兄弟节点p元素，+表示的是平级的兄弟关系</td>
</tr>
<tr>
<td>div#container&gt;ul</td>
<td>选取父级是div，且id是container的ul子元素，&gt;在这只去选择子代</td>
</tr>
<tr>
<td>p~ul</td>
<td>选取p元素后面的每一个ul元素</td>
</tr>
<tr>
<td>a[little]</td>
<td>选取有title属性的a元素</td>
</tr>
<tr>
<td>a[href=”http”]</td>
<td>选取href属性是http的a元素</td>
</tr>
<tr>
<td>a[href*=”ham”]</td>
<td>选取href属性中包含ham的a元素</td>
</tr>
<tr>
<td>a[href^=”http”]</td>
<td>选取href属性开头是http的a元素</td>
</tr>
<tr>
<td>a[href$=”ham”]</td>
<td>选取href属性结尾是ham的a元素</td>
</tr>
<tr>
<td>input[type=radio]:checked t</td>
<td>选取被选中的单选按钮的元素</td>
</tr>
<tr>
<td>div:not(#container)</td>
<td>选取所有id不是container的div元素</td>
</tr>
<tr>
<td>li:nth-child(3)</td>
<td>选取第三个li元素</td>
</tr>
<tr>
<td>li:nth-child(2n)</td>
<td>选取第偶数个的li元素</td>
</tr>
<tr>
<td>a::attr(href)</td>
<td>获取a标签的href属性值</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用 xpath 和 css 选择器进行提取</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">    url = response.xpath(<span class="string">&#x27;//h2[@class=&quot;news_entry&quot;]/a/@href&#x27;</span>).extract_first(<span class="string">&quot;&quot;</span>)</span><br><span class="line">    url = response.css(<span class="string">&#x27;//h2[class=&quot;news_entry&quot;] a::attr(href)&#x27;</span>).extract_first(<span class="string">&quot;&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>​    因为 <strong>response.xpath 和 response.css 都会构建 Selector 选择器对象</strong>，会返回出一个满足匹配的所有元素集合，所以如果进行后续的每一个元素的单独处理就需要获取每一个元素。对于这个 selector 对象也可以使用 scrapy.Selector(response) 去自行构建，只需要将下载后的 html 文档作为 response 参数传入即可 </p>
<p>​    <strong><code>extract_first(&quot;&quot;)</code> 方法，表示返回从匹配的元素集合中第一个元素，这里还给了个空字符串的参数，表示如果没有获取到元素则给一个空字符</strong></p>
<p>​    <code>extract()</code> 方法，则是就是直接返回列表，不过要注意使用 extract 后通过索引方式获取第一个节点，如果是空列表会抛出异常，所以只为了获取第一个节点元素推荐使用 extract_first 方法</p>
<p>​    另外，<strong>在爬取页面时，需要将 settings 配置文件的 ROBOTSTXT_OBEY 的值改为 False</strong>，因为有些网站会有 robots 一些，规定了网站的哪些页面是拒绝爬取的，可以直接在网站的 url 后，加上 “robots.txt” 查看。所以 scrapy 默认是 ROBOTSTXT_OBEY = True，会遵守这个 robots 协议，那可能会导致某些内容不能爬取，所以需要将个参数的值改为 False</p>
<h3 id="跟进爬取"><a href="#跟进爬取" class="headerlink" title="跟进爬取"></a>跟进爬取</h3><p>​    在 spider 中爬虫类默认只给了一个 parse 的方法去对 response 进行提取，但是通常来说，都需要对其中有用的 url 去进行跟进爬取来获得更多数据。常见的比如分页，当给定一个初始页的 url 之后，需要对后续的页面也进行提取，所以在 parse 的方法中就可以使用 python 的 yield 方式抛出请求让引擎去继续调度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以博客园的新闻页为例</span></span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Request   <span class="comment"># 或则 from scrapy.http import Request</span></span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> parse</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        1.获取新闻列表页中的新闻url并交给scrapy进行下载后调用对应的解析方法</span></span><br><span class="line"><span class="string">        2.获取下一页的url并交给scrapy下载，完成后交给parse继续跟进</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># next_url = response.css(&quot;div.pager a:last-child::text&quot;).extract_first(&quot;&quot;)  # css选择器提取文本使用 ::text</span></span><br><span class="line">        <span class="comment"># if next_url == &quot;Next &gt;&quot;:</span></span><br><span class="line">        <span class="comment">#     next_url = response.css(&quot;div.pager a:last-child::attr(href)&quot;).extract_first(&quot;&quot;)</span></span><br><span class="line">        <span class="comment">#     yield Request(parse.urljoin(response.url, next_url), callback=self.parse)</span></span><br><span class="line"></span><br><span class="line">        next_url = response.xpath(<span class="string">&quot;//a[contains(text(), &#x27;Next &gt;&#x27;)]/@href&quot;</span>).extract_first(<span class="string">&quot;&quot;</span>)</span><br><span class="line">        <span class="keyword">yield</span> Request(parse.urljoin(response.url, next_url), callback=self.parse)</span><br></pre></td></tr></table></figure>
<p>​    当 response 在 spider 进行解析时，如果最后使用 yield 返回，那么 yield 的 Request 对象会立即发给引擎然后紧接着继续进行解析操作。另外，这里的 Request 的对象中包含一个必要的参数 callback，这个参数用来设置得到 response 后是到哪一个解析方法中进行处理，因为这里只是提取下一页所以依然回调的是 parse 方法，如果是要对详情页的分析，就需要自定义一个解析方法专门处理对应的 response 对象进行解析</p>
<p>​    对于现在很多数据都是通过请求等方式进行动态获取的，也就意味着使用初始的 start_urls 中下载得到的 response 实际上不会包含需要的动态数据，那么就需要通过在浏览器的开发者工具里去判断哪一个请求可能是包含这些数据的，得到 url 后再去进行拼接并发起请求，这样就可以得到 json 序列化的动态数据</p>
<p>​    但是已经说了 <strong>scrapy 是一个异步框架，那么如果使用 Request 去封装发起请求，就会将 parse 方法变成了同步代码，也就会造成阻塞</strong>。所以 parse 的 yield 就又一次凸显功能了，<strong>利用 yield 返回，通过 callback 去执行回调，那么 yield 返回的 Request 就会利用到异步</strong>，但与此同时，编写就会更加麻烦，需要多编写其他的解析方法，同时考虑到持久化，还需要将已得到的数据传递到之后的解析方法中</p>
<p>​    需要注意的是，这里使用了 urljoin 这个方法，目的是为了进行路由的拼接，因为有些网站中，比如图片的 href 属性中其实只会有一个子路由，所以如果是要用跳到这个图片地址，需要手动拼接上域名，得到完整的 url</p>
<p>​    一种方法就是判断是否分析得到的头部是否含有 http 或 https，然后再手动用字符串拼接成完整 url。这里则是用 <strong>urllib 的 parse 模块，其中的 urljoin 方法可以自行去判断，如果 url 中有域名不会改动第二个 url 参数，如果没有则会将参数中的域名和 url 拼接</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> parse</span><br><span class="line"></span><br><span class="line">base_url = <span class="string">&#x27;https://www.wei-foun.github.io/article/123&#x27;</span></span><br><span class="line">path_url = <span class="string">&#x27;/news/12333&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(parse.urljoin(base_url, path_url))    <span class="comment"># https://www.wei-foun.github.io/news/12333</span></span><br><span class="line"></span><br><span class="line">path_url = <span class="string">&#x27;https://www.wei-foun.github.io/news/12333&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(parse.urljoin(base_url, path_url))    <span class="comment"># https://www.wei-foun.github.io/news/12333</span></span><br></pre></td></tr></table></figure>
<p>​    <strong>注意：上面的 path_url 中第一个字符是 ‘/‘，如果使用 urljoin 方法，path_url 的第一个字符不是 ‘/‘，会造成 urljoin 直接将 path_url 当做子路径加在 base_url 之后，例如 <code>https://www.wei-foun.github.io/article/123/news/12333</code></strong></p>
<h3 id="命令行的调试"><a href="#命令行的调试" class="headerlink" title="命令行的调试"></a>命令行的调试</h3><p>​    命令行 cd 进入到项目录下，输入  <code>scrapy shell 完整的 url</code>。既然 pycharm 可以有办法调试，为什么还要使用命令行呢？因为在 pycharm 中定义的 main 调试文件，每一次去 debug 时都会重新去发起一个请求来得到响应的结果，这意味着如果频繁在 pycharm 中边写边 debug 容易出现被服务端发现然后被禁的问题。所以对于解析的步骤如果要进行调试判断，可以在命令行使用 shell 参数去获取 response 对象，以此在命令行里使用 xpath 或 css 去进行元素解析，这样相比在 pycharm 中 debug 会去发新请求而言，使用 shell 参数去获取响应后，去测试是否正确解析到元素时，不会导致反复的发送请求</p>
<h3 id="item-数据传递"><a href="#item-数据传递" class="headerlink" title="item 数据传递"></a>item 数据传递</h3><p>​    scrapy 的 items 可以理解成对爬取数据进行持久化保存的时候前对所有提取数据的容器，类似字典的形式存储。而且 <strong>items 中只有一种类型字段就是 Field()</strong></p>
<p>​    对于需要多次 yield 来提交 Request 给到下载器进行像动态数据或是详情页面的数据页面，通常需要将已经解析得到的数据进行传递，对于创建的 spider 类中 parse 方法，yield 可以将请求返回，也可以直接返回 item。如果是 yield Request 则就是交给下载器去下载，yield item 对象时，则会将这个 item 对象给到 pipelines.py 做后续持久化相关操作</p>
<p>​    使用命令创建一个 spider 时，items.py 会自动生成一个 item 的类，<strong>默认继承 scrapy.Item</strong>。那么就只需要在这里将需要的信息创建一个字段即可</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ArticlespiderItem</span>(scrapy.Item):</span><br><span class="line">    artimg_url = scrapy.Field()</span><br><span class="line">    artimg_path = scrapy.Field()</span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    create_date = scrapy.Field()</span><br><span class="line">    content = scrapy.Field()</span><br><span class="line">    tags = scrapy.Field()</span><br><span class="line">    comment_num = scrapy.Field()</span><br><span class="line">    view_num = scrapy.Field()</span><br><span class="line">    url = scrapy.Field()</span><br><span class="line">    url_object_id = scrapy.Field()</span><br></pre></td></tr></table></figure>
<p>​    那么在 spider 类中，只需要导入整个 items 的模块，实例化这个类后，就可以将解析到的数据赋值给这个 item，然后在 yield 时，使用 meta 参数将这个 item 通过字典传递</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> items <span class="keyword">import</span> ArticlespiderItem</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">	...</span><br><span class="line">    artice_item = ArticlespiderItem()    <span class="comment"># 实例化定义的 item 对象，将解析的内容赋值给属性</span></span><br><span class="line">    artice_item[title] = response.xpath(<span class="string">&quot;//*[@id=&#x27;news_title&#x27;]//a/text()&quot;</span>).extract_first(<span class="string">&quot;&quot;</span>)</span><br><span class="line">    ...</span><br><span class="line">	<span class="keyword">yield</span> Request(url=url, meta=&#123;<span class="string">&quot;artice_item&quot;</span>,artice_item&#125;, callback=parse_detail)</span><br><span class="line">    <span class="comment"># meta 参数可以将 item 对象继续往后续的请求传递，让后面的解析使用</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse_detail</span>(<span class="params">self, response</span>):</span><br><span class="line">    artice_item = response.meta.get(<span class="string">&quot;artice_item&quot;</span>, <span class="string">&quot;&quot;</span>)   <span class="comment"># response.meta.get 获取携带的 item 对象</span></span><br><span class="line">    content = response.xpath(<span class="string">&quot;//*[@id=&#x27;news_content&#x27;]&quot;</span>).extract()[<span class="number">0</span>]</span><br><span class="line">    artice_item[content] = content</span><br><span class="line">    <span class="keyword">yield</span> artice_item</span><br></pre></td></tr></table></figure>
<h3 id="图片自动下载"><a href="#图片自动下载" class="headerlink" title="图片自动下载"></a>图片自动下载</h3><p>​    scrapy 本身提供了对图片和文件的自动下载，前提需要在 items 类中创建两个字段   <code>image_urls = scrapy.Field()</code> 和 <code>images = scrapy.Field()</code>，前者用来存储图片的 url，后者用来保存图片数据</p>
<p>​    然后， <strong>settings 配置中设置 <code>ITEM_PIPELINES = &#123;&#39;scrapy.contrib.pipeline.images.ImagesPipeline&#39;: 1&#125;</code>，同时需要配置一个图片的保存目录 <code>IMAGES_STORE = &#39;/path/to/valid/dir&#39;</code></strong></p>
<p>​    <strong>注意，在 parse 方法中给 item 实例的 image_urls 必须是一个 列表</strong>，当 spider 执行到管道时，会进入到启用的 ImagesPipeline，会去对 item 实例的 image_urls 进行遍历，取出图片的 url ，交给下载器去下载</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> response.meta.get(<span class="string">&quot;artimg_url&quot;</span>, <span class="string">&quot;&quot;</span>):</span><br><span class="line">	<span class="comment"># 判断封面图是否存在，存在就加入到列表中，因为使用自动爬取图片时，会进行循环，直接是否字符串会引发错误</span></span><br><span class="line">	article_item[<span class="string">&quot;artimg_url&quot;</span>] = [response.meta.get(<span class="string">&quot;artimg_url&quot;</span>, <span class="string">&quot;&quot;</span>)]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">	<span class="comment"># 不存在，直接赋空列表</span></span><br><span class="line">	article_item[<span class="string">&quot;artimg_url&quot;</span>] = []</span><br></pre></td></tr></table></figure>
<p>​    如果 image_urls 字段的值不是列表类型，只是一个 url 的字符串，那么遍历就只会得到第一个字符，并抛出一个 ValueError</p>
<p>​    不过，通常情况下不会直接在配置文件中直接复制图片目录的绝对路径，所以可以通过创建 main 调试文件那样，在 settings 文件中导入根目录，然后和图片目录进行拼接</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os, sys</span><br><span class="line"></span><br><span class="line">project_img = os.path.abspath(os.path.dirname(__file__))</span><br><span class="line">IMAGES_STORE = os.path.join(project_img, <span class="string">&quot;images&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>​    同样，根据需要可以对自带的 ImagesPipeline 进行定制，可以在 pipelines 文件中自定义一个 pipeline 类去继承，同时需要将 settings 的 <code>ITEM_PIPELINES</code> 中的配置改成自己设置的 pipeline 类 <code>&#39;ArticleSpider.pipelines.ArticleImgPipeline&#39;: 1</code>，<strong>配置后面的数字表示优先级，数字越小，越优先执行</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.pipelines.images <span class="keyword">import</span> ImagesPipeline</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ArticleImgPipeline</span>(<span class="title class_ inherited__">ImagesPipeline</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;自动去下载文章封面图，并获取图片的保存路径&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">item_completed</span>(<span class="params">self, results, item, info</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&quot;artimg_url&quot;</span> <span class="keyword">in</span> item:</span><br><span class="line">            artimg_path = <span class="string">&quot;&quot;</span></span><br><span class="line">            <span class="keyword">for</span> ok, value <span class="keyword">in</span> results:</span><br><span class="line">                artimg_path = value[<span class="string">&quot;path&quot;</span>]    <span class="comment"># 获取图片的保存路径</span></span><br><span class="line">            item[<span class="string">&quot;artimg_path&quot;</span>] = artimg_path</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> item   <span class="comment"># pipeline 类中方法最后一定要将 item 进行返回，否则其余开启的 pipeline 就无法获取这个 item</span></span><br></pre></td></tr></table></figure>
<p>​    当图片的 url 发出请求后，会被管道处理，进入到 item_completed 方法中，图片的下载会返回 True 和 False 来表示，另外 image_info_or_error 的字典，也就是参数的 results 会包含三个字段，url 表示图片的 url，path 是图片保存的路径，checksum 是图片的 md5 值</p>
<h3 id="Json-文件存储"><a href="#Json-文件存储" class="headerlink" title="Json 文件存储"></a>Json 文件存储</h3><p>​    同样需要在 pipelines 文件中去实现，并在 settings 中进行配置，对于 json 方式保存数据，可以自己定义，也可以使用 scrapy 自带的专门用于 json 存储的 pipeline</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">JsonWithEncodingPipeline</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将解析的item数据存储在json文档中&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.f = codecs.<span class="built_in">open</span>(<span class="string">&quot;article.json&quot;</span>, <span class="string">&quot;a&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">        <span class="comment"># codecs 是一个内置库，使用 codecs.open 来打开文件时，会自动将内容先转换为 unicode 编码，然后根据所给的编码参数，将内容编码到文件</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        lines = json.dumps(<span class="built_in">dict</span>(item), ensure_ascii=<span class="literal">False</span>) + <span class="string">&quot;\n&quot;</span></span><br><span class="line">        <span class="comment"># ensure_ascii=False 是为了防止出现中文乱码</span></span><br><span class="line">        self.f.write(lines)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        self.f.close()</span><br></pre></td></tr></table></figure>
<p>​    一般操作文件会使用两个方法：<strong>open_spider(self, spider)</strong> 和 <strong>close_spider(self, spider)</strong>，分别用来开启文件上下文，和操作完成后关闭文件。如果要对 item 中数据保存到 json，需要先将 item 转换为 字典 形式，才能使用 json.dumps</p>
<p>​    scrapy 中也提供了  <strong>JsonExporterPipeline</strong> ，这个类用于存储 json 的内容 <code>from scrapy.exporters import JsonItemExporter</code>，除此之外还有其他各种文件类型存储的 pipeline</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.exporters <span class="keyword">import</span> JsonItemExporter</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">JsonExporterPipeline</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用自带的export的一些方法实现对items的json序列化&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.f = <span class="literal">None</span></span><br><span class="line">        self.exporter = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">open_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        self.f = <span class="built_in">open</span>(<span class="string">&quot;article_exporter.json&quot;</span>, <span class="string">&quot;wb&quot;</span>)</span><br><span class="line">        self.exporter = JsonItemExporter(self.f, encoding=<span class="string">&quot;utf8&quot;</span>, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line">        self.exporter.start_exporting()  <span class="comment"># 自带的 item export 需要调用 start_exporting() 开始</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        self.exporter.export_item(item)  <span class="comment"># 并通过 export_item 方法将 item 参数序列化，并写入文件</span></span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        self.exporter.finish_exporting() <span class="comment"># 最后调用 finish_exporting() 来关闭 export</span></span><br><span class="line">        self.f.close()</span><br></pre></td></tr></table></figure>
<p>​    对于所有的 item exporter，scrapy  都提供了三个方法进行一次完整的处理，但是调用前，需要先实例化使用的 item exporter 类</p>
<p>​    1）调用方法   <code>start_exporting()</code> 以标识 exporting 过程的开始</p>
<p>​    2）对要导出的每个项目调用   <code>export_item()</code> 方法，并将需要序列化的 item 作为参数传入</p>
<p>​    3）最后调用   <code>finish_exporting()</code> 表示 exporting 过程的结束</p>
<p>​    另外 JsonItemExporter 这个类，会将所有的字典结果都放在一个列表里，JsonLinesItemExporter 则是将每一个 json 后的字典单独放入文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># JsonItemExporter </span><br><span class="line">[&#123;&quot;name&quot;: &quot;Color TV&quot;, &quot;price&quot;: &quot;1200&quot;&#125;,</span><br><span class="line">&#123;&quot;name&quot;: &quot;DVD player&quot;, &quot;price&quot;: &quot;200&quot;&#125;]</span><br><span class="line"></span><br><span class="line"># JsonLinesItemExporter </span><br><span class="line">&#123;&quot;name&quot;: &quot;Color TV&quot;, &quot;price&quot;: &quot;1200&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;: &quot;DVD player&quot;, &quot;price&quot;: &quot;200&quot;&#125;</span><br></pre></td></tr></table></figure>
<p>​    注意，对于主体逻辑部分的 process_item 方法，结束是必须将 item 返回，因为在配置文件中   <code>ITEM_PIPELINES</code> 可能配置多个 pipeline，所以如果不返回 item 其余的管道就不能进一步做后续处理</p>
<p>​    但是如果明确不希望返回 item 让后续 pipeline 去执行自己的 process_item 方法，<strong>通过导入 <code>from scrapy.exception import DropItem</code> 可以抛出 <code>DropItem()</code> 异常，将当前 item 丢弃，这样其余的 pipeline 就不会去执行自己的 process_item 方法来处理</strong></p>
<h3 id="数据持久化"><a href="#数据持久化" class="headerlink" title="数据持久化"></a>数据持久化</h3><p>​    以最常见的 mysql 为例，在 pipeline 中就要运用 mysql 的第三方库去实现数据的插入，最常见的库有 pymysql 和 mysqlclient，两个都是由统一作者编写的。pymysql 是纯 Python 实现的，安装和使用都方便，但是插入效率上要比 mysqlclient 慢；<strong>mysqlclient 在 Python3 中的包名是 MySQLdb，是一个 C 语言扩展模块</strong>，在安装编译上会有一些问题，但是在插入效率上基本和 mysql 的效率一致</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install -i https://pypi.douban.com/simple pymysql</span><br><span class="line">pip install -i https://pypi.douban.com/simple mysqlclient</span><br></pre></td></tr></table></figure>
<p>​    使用 pip 去安装 mysqlclient 时，可能会出现安装问题，可以直接从  <code>https://www.lfd.uci.edu/~gohlke/pythonlibs/</code> 网站中搜索 mysqlclient 选择匹配的版本进行下载，然后直接使用 <code>pip install [下载的 whl 文件]</code> 也能实现安装</p>
<p>​    有了第三方库的支持，接下来的 mysql 写入就和之前的 json 文件持久化类似了，根据提供的方法创建连接并插入执行 sql 语句</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> MySQLdb    <span class="comment"># 注意 mysqlclient 的包名是 MySQLdb</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MysqlPipeline</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;连接mysql进行数据存储&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.conn = <span class="literal">None</span></span><br><span class="line">        self.cur = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">open_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        self.conn = MySQLdb.connect(       <span class="comment"># 使用 MySQLdb.connect 建立 mysql 连接</span></span><br><span class="line">            host=<span class="string">&quot;127.0.0.1&quot;</span>,</span><br><span class="line">            port=<span class="number">3306</span>,</span><br><span class="line">            user=<span class="string">&quot;root&quot;</span>,</span><br><span class="line">            password=<span class="string">&quot;123456&quot;</span>,</span><br><span class="line">            db=<span class="string">&quot;test_spider&quot;</span>,</span><br><span class="line">            charset=<span class="string">&quot;utf8&quot;</span>,                <span class="comment"># 设置编码</span></span><br><span class="line">            use_unicode=<span class="literal">True</span>               <span class="comment"># 如果设置了 charset 则会默认使用 use_unicode=True，作用是对文本内容进行编码，True 表示将内容作为 unicode 对象，False 则是作为字符串对象</span></span><br><span class="line">        )</span><br><span class="line">        self.cur = self.conn.cursor()      <span class="comment"># 创建游标</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        insert_sql = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            insert into cnblogs_article(url_object_id, title, url, artimg_url, artimg_path, tags, comment_num,</span></span><br><span class="line"><span class="string">            view_num, content, create_date) </span></span><br><span class="line"><span class="string">            values(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        params = []</span><br><span class="line">        params.append(item.get(<span class="string">&quot;url_object_id&quot;</span>, <span class="string">&quot;&quot;</span>))</span><br><span class="line">        params.append(item.get(<span class="string">&quot;title&quot;</span>, <span class="string">&quot;&quot;</span>))</span><br><span class="line">        params.append(item.get(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;&quot;</span>))</span><br><span class="line">        params.append(<span class="string">&#x27;,&#x27;</span>.join(item.get(<span class="string">&quot;artimg_url&quot;</span>, [])))</span><br><span class="line">        params.append(item.get(<span class="string">&quot;artimg_path&quot;</span>, <span class="string">&quot;&quot;</span>))</span><br><span class="line">        params.append(item.get(<span class="string">&quot;tags&quot;</span>, <span class="string">&quot;&quot;</span>))</span><br><span class="line">        params.append(item.get(<span class="string">&quot;comment_num&quot;</span>, <span class="number">0</span>))</span><br><span class="line">        params.append(item.get(<span class="string">&quot;view_num&quot;</span>, <span class="number">0</span>))</span><br><span class="line">        params.append(item.get(<span class="string">&quot;content&quot;</span>, <span class="string">&quot;&quot;</span>))</span><br><span class="line">        params.append(item.get(<span class="string">&quot;create_date&quot;</span>, <span class="string">&quot;1970-07-01&quot;</span>))</span><br><span class="line">        self.cur.execute(insert_sql, <span class="built_in">tuple</span>(params))     <span class="comment"># cur.execute 来执行 sql</span></span><br><span class="line">        self.conn.commit()                 <span class="comment"># 用 conn 去 commit 提交事务</span></span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        self.cur.close()</span><br><span class="line">        self.conn.close()</span><br></pre></td></tr></table></figure>
<h4 id="主键冲突"><a href="#主键冲突" class="headerlink" title="主键冲突"></a>主键冲突</h4><p>​    注意：在数据库建表的时候，这里是将 url_object_id 用 md5 方法将一个随机值作为了主键。当 spider 在 debug 或二次爬取时，数据库就会存在与插入语句的 url_object_id 相同的数据，这时就会抛出主键冲突的异常，因此需要对 insert 的 sql 语句进行修改，一旦数据已经存在，就对数据中的某些字段进行更新操作</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># on duplicate key update 是 mysql 的特有语法，目的是在进行数据插入时相同数据会出现的异常情况下对数据进行更新操作</span></span><br><span class="line">insert_sql = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            insert into cnblogs_article(url_object_id, title, url, artimg_url, artimg_path, tags, comment_num,</span></span><br><span class="line"><span class="string">            view_num, content, create_date) </span></span><br><span class="line"><span class="string">            values(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)</span></span><br><span class="line"><span class="string">            on duplicate key update </span></span><br><span class="line"><span class="string">            view_num=values(view_num)，</span></span><br><span class="line"><span class="string">            comment_num=values(comment_num);</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<h4 id="异步入库"><a href="#异步入库" class="headerlink" title="异步入库"></a>异步入库</h4><p>​    上面使用 mysqlclient 已经可以完成对数据的持久化，但是因为 scrapy 是异步框架，对于 mysqlclient 来说这样的插入依然是同步代码，scrapy 的爬取速度远远大于将数据插入到数据库的速度，所以可以对同步的入库进行优化改为异步方式。<strong>twisted 的 enterprise 模块提供一个 adbapi 的接口，专门用于实现数据的异步入库</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> twisted.enterprise <span class="keyword">import</span> adbapi</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MysqlTwistedPipeline</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用twisted实现异步状态录入数据到mysql数据库&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dbpool</span>):</span><br><span class="line">        self.dbpool = dbpool</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">from_settings</span>(<span class="params">cls, settings</span>):</span><br><span class="line">        <span class="comment"># 使用settings文件，定义配置变量</span></span><br><span class="line">        <span class="keyword">from</span> MySQLdb.cursors <span class="keyword">import</span> DictCursor</span><br><span class="line">        db_params = <span class="built_in">dict</span>(</span><br><span class="line">            host=settings[<span class="string">&quot;MYSQL_HOST&quot;</span>],</span><br><span class="line">            user=settings[<span class="string">&quot;MYSQL_USER&quot;</span>],</span><br><span class="line">            password=settings[<span class="string">&quot;MYSQL_PASSWORD&quot;</span>],</span><br><span class="line">            port=<span class="number">3306</span>,</span><br><span class="line">            db=settings[<span class="string">&quot;MYSQL_DBNAME&quot;</span>],</span><br><span class="line">            use_unicode=<span class="literal">True</span>,</span><br><span class="line">            charset=<span class="string">&quot;utf8&quot;</span>,</span><br><span class="line">            cursorclass=DictCursor   <span class="comment"># 以字典的形式将记录返回显示，默认使用的元组形式返回</span></span><br><span class="line">        )</span><br><span class="line">        db_pool = adbapi.ConnectionPool(<span class="string">&quot;MySQLdb&quot;</span>, **db_params)  <span class="comment"># 使用 adbapi 连接数据库，创建连接池</span></span><br><span class="line">        <span class="keyword">return</span> cls(db_pool)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        query = self.dbpool.runInteraction(self.do_insert, item)</span><br><span class="line">        <span class="comment"># dbpool.runInteraction 方法会以异步方式调用 do_insert 函数，并将参数 item 传给 do_insert 函数</span></span><br><span class="line"></span><br><span class="line">        query.addErrback(self.handle_error, item, spider)</span><br><span class="line">        <span class="comment"># addErrback 是执行插入时如果报错则调用 handle_error 的函数用来查看错误</span></span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">handle_error</span>(<span class="params">self, failure, item, spider</span>):</span><br><span class="line">        <span class="comment"># 错误回调函数的第一种参数 failure 是默认传递的，表示错误信息，其余参数则可根据需要添加，后面 item 和 spider 可根据需要传递</span></span><br><span class="line">        <span class="built_in">print</span>(failure)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">do_insert</span>(<span class="params">self, cursor, item</span>):</span><br><span class="line">        <span class="comment"># 这里把和之前同步入库的 sql 全部放在了 item 中的 get_insert_sql 方法里，该方法只需返回 insert_sql, params 去交给 cursor 进行执行即可</span></span><br><span class="line">        insert_sql, params = item.get_insert_sql()</span><br><span class="line">        cursor.execute(insert_sql, params)</span><br></pre></td></tr></table></figure>
<p>​    这个类中使用一个 <strong>类方法 from_setting，默认需要一个参数 setting，表示的就是全局配置的 settings 文件</strong>。利用这个方法，可以将数据库的连接配置放在 settings 文件中，实现配置信息与逻辑代码分离</p>
<p>​    <strong>twisted 的 adbapi 提供了异步入库的支持，首先需要通过 adbapi.ConnectionPool 来创建一个连接池，第一个参数是一个字符串表示使用的 mysql 模块，第二个参数则是数据库的连接配置设置</strong>。然后使用 cls 方法实例化连接池对象并返回，那么在 pipeline 类的 init 方法中既可以得到这个异步连接池对象</p>
<p>​    process_item 方法中，只需要使用得到的连接池对象 self.dbpool 去调用 runInteraction 方法，然后连接池对象会自动传入 cursor 给第一个参数 do_insert 的方法，同时需要将 item 给这个方法，对于 sql 的执行就可以在 do_insert 中实现</p>
<p>​    针对入库中可能遇到的错误，可以将 runInteraction 方法的结果赋值给 query，通过 <strong>query.addErrback() 方法对异常处理进行回调，第一个参数是出错的回调方法</strong>，item 和 spider 可根据需要传递，一旦出现异常错误，信息会赋给参数 failure，这个参数是定义 addErrback 的回调方法时，会自动进行传递的</p>
<h3 id="item-loaders"><a href="#item-loaders" class="headerlink" title="item loaders"></a>item loaders</h3><p>​    直接在 response 上使用 xpath 和 css 进行解析提取的元素，返回结果都是一个列表的形式，因此会有很多解析都会使用到 extract_first 这样的方法，并且对于一些解析后的字符串，甚至还需要利用正则或是其他方式再去进行一个提取，这就使得 parse 中的解析操作会随着需要提取的数据增多，而导致这个方法的内容非常繁琐</p>
<p>​    scrapy 提供的 Item Loaders 就可以很好的对解析提取进行一个优化，并且也方便扩展和维护。简单来说，ItemLoader 实现了原本 页面解析 与 item 实例对象属性赋值 的整合</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.contrib.loader <span class="keyword">import</span> ItemLoader</span><br><span class="line"><span class="keyword">from</span> myproject.items <span class="keyword">import</span> Product</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">    l = ItemLoader(item=Product(), response=response)</span><br><span class="line">    l.add_xpath(<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;//div[@class=&quot;product_name&quot;]&#x27;</span>)</span><br><span class="line">    l.add_xpath(<span class="string">&#x27;price&#x27;</span>, <span class="string">&#x27;//p[@id=&quot;price&quot;]&#x27;</span>)</span><br><span class="line">    l.add_css(<span class="string">&#x27;stock&#x27;</span>, <span class="string">&#x27;p#stock]&#x27;</span>)</span><br><span class="line">    l.add_value(<span class="string">&#x27;last_updated&#x27;</span>, <span class="string">&#x27;today&#x27;</span>) <span class="comment"># you can also use literal values</span></span><br><span class="line">    <span class="keyword">return</span> l.load_item()</span><br></pre></td></tr></table></figure>
<p>​    <strong>导入 ItemLoader 类后，需要进行实例化，第一个参数就是 item，第二个参数则是下载器得到的 response</strong>。从官网的例子可以看到，这个实例对象可以通过 <strong>add_xpath，add_css，add_value</strong> 三种方式去数据获取，比如要获取 url，<code>l.add_value(&quot;url&quot;, response.url)</code>，且 <strong>默认得到的形式都是一个列表的形式</strong></p>
<p>​    当数据都被解析后，需要   <strong>调用 ItemLoader 的 <code>load_item()</code> 方法将数据填充到 item 中</strong>，最后只需要将这个 item 对象返回即可</p>
<h4 id="Field-的定制"><a href="#Field-的定制" class="headerlink" title="Field 的定制"></a>Field 的定制</h4><p>​    上面使用 item loader 去自动解析数据，但获取的都是列表对象，这样形式并不能直接入库，因此就需要对 item 中的内容做进一步的处理，<strong>items 中 <code>scrapy.Field()</code> 的字段都其实包含两个参数，input_processor 和 output_processor</strong> 两个处理器，来对进入和输出进行各自的处理</p>
<p>​    通常情况下，item loader 解析获取的列表的内容会需要进行多种操作，因此 <strong>scrapy 的 processor 提供了一个类 MapCompose，它可以接受多个参数（函数），并且在处理时，会按照顺序依次初始的列表数据通过设定函数做处理</strong></p>
<p>​    除此之外，对于大部分只有一个元素的列表而言，只需要提取第一个字符串元素这样的 item，processor 也提供了一个 TakeFirst 类，方便做提取</p>
<p>​    另外，对于含有多个元素的列表，如果是要通过     <code>&#39;,&#39;.join()</code> 来做一个拼接的话，processor 中也有 <code>Join</code> 来做这类的处理</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.loader.processors <span class="keyword">import</span> TakeFirst，MapCompose, Join</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">filter_price</span>(<span class="params">value</span>):</span><br><span class="line">    <span class="keyword">if</span> value.isdigit():</span><br><span class="line">        <span class="keyword">return</span> value</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Product</span>(scrapy.Item):</span><br><span class="line">    name = scrapy.Field(</span><br><span class="line">        input_processor=MapCompose(remove_tags),</span><br><span class="line">        output_processor=Join(),</span><br><span class="line">    )</span><br><span class="line">    price = scrapy.Field(</span><br><span class="line">        input_processor=MapCompose(remove_tags, filter_price),</span><br><span class="line">        output_processor=TakeFirst(),</span><br><span class="line">    )</span><br><span class="line">--------------------------------------------------------------------------------------</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> scrapy.contrib.loader <span class="keyword">import</span> ItemLoader</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>il = ItemLoader(item=Product())</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>il.add_value(<span class="string">&#x27;name&#x27;</span>, [<span class="string">u&#x27;Welcome to my&#x27;</span>, <span class="string">u&#x27;&lt;strong&gt;website&lt;/strong&gt;&#x27;</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>il.add_value(<span class="string">&#x27;price&#x27;</span>, [<span class="string">u&#x27;&amp;euro;&#x27;</span>, <span class="string">u&#x27;&lt;span&gt;1000&lt;/span&gt;&#x27;</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>il.load_item()</span><br><span class="line">&#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">u&#x27;Welcome to my website&#x27;</span>, <span class="string">&#x27;price&#x27;</span>: <span class="string">u&#x27;1000&#x27;</span>&#125;</span><br></pre></td></tr></table></figure>
<p>​    如果自定义的 item 类中很多字段其实都是只要从列表中提取第一个，虽然给每一个 Field 中添加一个 TakeFirst 方法可以完成，但这就显得代码不够简洁。因此可以通过继承提供的 ItemLoader 类去修改默认的使用方法可以更方便的去做修改</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ArticleItemLoader</span>(<span class="title class_ inherited__">ItemLoader</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;自定义itemloader类，让所有item类型都只取第一个，即每一个值的类型都是str&quot;&quot;&quot;</span></span><br><span class="line">    default_output_processor = TakeFirst()</span><br></pre></td></tr></table></figure>
<p>​    <strong>ItemLoader 类中默认的 input_processor 和 output_processor 使用都是 Identity()，也就是返回一个列表形式</strong>。通过修改 <code>default_output_processor</code> 就可以让 item 每一个字段都默认去提取列表的第一个字符串元素，于此同时，在 parse 方法中修改更改实例的 ItemLoader 改为自定义的 ArticleItemLoader 类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">    l = ArticleItemLoader(item=Product(), response=response)</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<p>​    通过这样的方法，item 每个字段都会变成 str 类型。但是对于 url 这样的类型，会需要遍历去给下载器，就必须要是原来的 list 类型，那就可以单独在 itme 类中，设置 Field 参数为默认的 Identity()</p>
<h3 id="scrapy-的-url-去重"><a href="#scrapy-的-url-去重" class="headerlink" title="scrapy 的 url 去重"></a>scrapy 的 url 去重</h3><p>​    默认在 scrapy.dupefilters 中有两个类，一个是 BaseDupeFilter，一个是基于 BaseDupeFilter 实现的 RFPDupeFIlter 类，这个类默认是实现 Base 中的方法完成对请求 URL 的去重，只需要在 <strong>settings 文件中配置 <code>DUPEFILTER_CLASS = &#39;scrapy.dupefilters.RFPDupeFilter&#39;</code></strong>，这样在发送请求下载时，就会自动对 url 去重，而 BaseDupeFilter 中的 request_seen 方法是去重的逻辑</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.utils.request <span class="keyword">import</span> request_fingerprint</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">request_seen</span>(<span class="params">self, request</span>):</span><br><span class="line">    fp = self.request_fingerprint(request)</span><br><span class="line">    <span class="keyword">if</span> fp <span class="keyword">in</span> self.fingerprints:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    self.fingerprints.add(fp)</span><br><span class="line">    <span class="keyword">if</span> self.file:</span><br><span class="line">        self.file.write(fp + os.linesep)</span><br></pre></td></tr></table></figure>
<p>​    调度器会执行 enqueue_requests 方法，在内部判断 dont_filter 这个参数以及判断 dupefilter 模块的 request_seen 方法</p>
<p>​    request_seen 中通过 <strong>request_fingerprint</strong> 方法对每一个请求的做一个 hashlib.sha1 编码处理，其中包含了请求方式，请求 url，请求体，对三者进行编码得到唯一标识</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fp = hashlib.sha1()</span><br><span class="line">fp.update(to_bytes(request.method))</span><br><span class="line">fp.update(to_bytes(canonicalize_url(request.url, keep_fragments=keep_fragments)))</span><br><span class="line">fp.update(request.body <span class="keyword">or</span> <span class="string">b&#x27;&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>​    并且加入到 fingerprints 这个集合中，每一个标识符会在 fingerprints 中判断，True 就说明是重复 url，跳过下载继续往后执行其他操作，False 则是将 url 加入到集合里</p>
<p>​    除此之外，也可以 <strong>在 yield 的 Request 时，设置 dont_filter = True 来实现去重</strong></p>
<h4 id="去重策略"><a href="#去重策略" class="headerlink" title="去重策略"></a>去重策略</h4><p>​    1）最方便的一种方式，就是直接将爬取的 url 入库，然后每一次对新的 url 都去到数据库中查询一遍，来判断是否已经存在。但显然这种方式虽然简单，但是效率并不高，需要遍历一遍表来查询，且数据量一旦变大，所占用的空间也会非常大</p>
<p>​    2）直接将 url 放到 set 集合中，这样只需要 O(1) 的代价就能查询到 url，但是这种方式也会随着数据量增多，而导致占用更多内存空间</p>
<p>​    3）将 url 进行算法编码，来生成一个固定长度的 hash 值，然后在放入到 set 中，这样相比直接保存 url 在空间占用上是成倍的减少</p>
<p>​    4）使用 bitmap 算法（位图算法），将 url 通过 hash 函数映射到某一位（bit）上，这样在存储空间占用上就是成几十倍的减小，能大大压缩使用的内存空间，但问题是 hash 函数会导致不同的 url 会映射在相同的位置上</p>
<p>​    5）使用 bloomfilter（布隆过滤器），它在 bitmap 上做了进一步的优化，<strong>使用了多重 hash 来降低可能的结果冲突</strong></p>
<h3 id="CrawlSpider"><a href="#CrawlSpider" class="headerlink" title="CrawlSpider"></a>CrawlSpider</h3><p>​    和 spider 一样都是 scrapy 中的爬虫类，但是 crawlspider 是 spider 的子类，相比 spider，它的功能要比父类更多</p>
<p>​    spider 类的设计原则只是为了获得 start_url 也就是给的起始网页链接返回的响应数据，crawlspider 类则是可以定义规则（rules）提供对初始 url 返回中所有可能进一步获取数据的 url 进行下载爬取，比如获取每一页的页码数据。spider 中本身也支持跟进下载爬取，只是说需要自己去实现在初始返回的 response 提取到跟进 url 并通过 yield Request 来将 url 交给下载器获取响应，但是在 <strong>CrawlSpider 中就可以利用规则中的设置 LinkExtra 链接提取器，去自动获取到符合规则的 url，来触发 callback 获取下载后的 response</strong></p>
<h4 id="创建一个-CrawlSpider"><a href="#创建一个-CrawlSpider" class="headerlink" title="创建一个 CrawlSpider"></a>创建一个 CrawlSpider</h4><p>​    使用命令行进行创建前，依然需要先 cd 到项目目录下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy genspider -t crawl 爬虫项目名称 域名</span><br></pre></td></tr></table></figure>
<pre><code>创建完成后在 spiders 目录下会创建对应的爬虫文件，且文件中的爬虫类继承 spiders 下的 CrawlSpider，同时在类中会有一个 rules 的元组，其中放置需要进行匹配的规则 Rule，以及一个 parse_item 方法</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.contrib.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"><span class="keyword">from</span> scrapy.contrib.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MySpider</span>(<span class="title class_ inherited__">CrawlSpider</span>):</span><br><span class="line">    name = <span class="string">&#x27;example.com&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;example.com&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;http://www.example.com&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    rules = (</span><br><span class="line">        Rule(LinkExtractor(allow=(<span class="string">&#x27;category\.php&#x27;</span>, ), deny=(<span class="string">&#x27;subsection\.php&#x27;</span>, )))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_item</span>(<span class="params">self, response</span>):</span><br><span class="line">		<span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p>​    需要注意的是 crawlspider 的类中已经实现了原本 spider 的 <strong>parse 方法</strong>，所以 <strong>在创建的 crawlspider 类中不需要去复写</strong> 该方法，<strong>否则运行会出错</strong>。crawlspider 中 parse 方法则是去调用 _parse_response(response, self.parse_start_url, cb_kwargs={}, follow=True) 这个方法</p>
<p>​    _parse_response 方法其中又会去调用 process_results，而默认 process_results 接受了两个参数 response 和 results，且直接将 results 返回，而这个 results 就是执行 callback 后的结果，所以也可以通过复写 process_results 方法来进行一些自定义</p>
<p>​    <strong>如果需要对 parse 去做处理，可以使用 parse_start_url 这个方法</strong>，这个方法和 parse 一样都会接受一个 response 参数，crawlspider 中 parse_start_url 方法默认是返回了一个空列表</p>
<h4 id="提取规则"><a href="#提取规则" class="headerlink" title="提取规则"></a>提取规则</h4><p>​    Rule 可以称作是规则提取器，有三个主要参数，第一个参数 <strong>link_extractor</strong> 这个代表就是链接提取器，第二个参数 <strong>callback</strong> 代表对匹配链接提取器的链接的响应内容进行解析的回调函数，第三个参数 <strong>follow</strong> 代表对满足匹配的响应中是否跟进其中再次符合链接提取器的内容，这个参数的值是一个布尔值，默认当使用 callback 参数时，follow 的值为 True</p>
<p>​    对于 <strong>LinkExtractor 链接提取对象，支持三种提取方式：正则（创建默认使用的是正则），css，xpath</strong>。allow 参数就是允许匹配的内容，deny 参数是拒绝匹配的内容，两者选一即可</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 正则提取：默认创建时会自动使用正则</span></span><br><span class="line">Rule(LinkExtractor(allow=<span class="string">r&#x27;Items/&#x27;</span>), callback=<span class="string">&#x27;parse_item&#x27;</span>, follow=<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line"><span class="comment"># xpath提取：</span></span><br><span class="line">Rule(LinkExtractor(restrict_xpaths=<span class="string">&#x27;//div[@class=&quot;page_num&quot;]/a&#x27;</span>), callback=<span class="string">&#x27;parse_item&#x27;</span>, follow=<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line"><span class="comment"># css提取：</span></span><br><span class="line">Rule(LinkExtractor(restrict_css=<span class="string">&#x27;.page_num &gt; a&#x27;</span>), callback=<span class="string">&#x27;parse_item&#x27;</span>, follow=<span class="literal">True</span>),</span><br></pre></td></tr></table></figure>
<p>​    对于一个网站来说，其中会包含很多不同的 url，所以在 Rule 中可以创建多个 link_extractor 提取对象，在匹配时则会按照定义的顺序，使用第一个满足匹配的 callback 去执行</p>
<h3 id="反爬机制与反爬策略"><a href="#反爬机制与反爬策略" class="headerlink" title="反爬机制与反爬策略"></a>反爬机制与反爬策略</h3><p>​    爬虫：自动获取（批量获取）网站数据的程序</p>
<p>​    反爬虫：使用技术手段阻止爬虫程序的方法和机制</p>
<p>​    误伤：服务端的反爬技术将普通用户识别成了爬虫，导致用户不能正常访问</p>
<p>​    成本：指网站为了应对爬虫程序，投入的人力以及机器成本</p>
<p>​    拦截：即对识别到的爬虫程序进行请求拦截，拦截率越高，也会导致误伤率较高</p>
<h4 id="频率"><a href="#频率" class="headerlink" title="频率"></a>频率</h4><p>​    对于一些网站可能会对爬虫做一些频率上的限制，如果爬虫程序执行太快，频繁的请求就会被服务端检测并标识为爬虫，可能就会出现非 200 的状态码，来要求做验证，或是对 ip 限制时间访问防止本机一定时间内对网站发起请求。不过对服务端而言，设置 ip 限制很容易造成误伤</p>
<p>​    所以对于频率的限制来说，就需要对爬虫程序做一些改进，如果是对 ip 做限制（一般都不会 ip 做限制，因为局部网来说，对外 ip 只有几个），则可以用一些代理通过使用这些 ip 去对网站发请求，但这种方式对于一些小服务来说也会有损害，因为异步的 scrapy 可以短时间发起大量的请求，小型的服务可能在并发上不会做过多设置，会导致给服务端很大的压力</p>
<p>​    除了使用代理，scrapy 中还可以通过设置请求间隔的时间来做到不会频繁发起请求，而被检测到为爬虫。可以在 settings 中配置 <code>DOWNLOAD_DELAY=10</code> ，可以实现每隔 10 秒才发起一次请求，这个具体时间可以根据服务端的设置来设定</p>
<p>​    但是这个配置也依然可能会被检测到，因为服务端收到的请求的间隔时间都是完全一致 10 秒，也可能被认为是爬虫，所以 scrapy 还有另外一个配置 <code>RANDOMIZE_DOWNLOAD_DELAY=True</code>，该配置会在 0.5 - 1.5 之间产生一个随机值并乘以 <code>DOWNLOAD_DELAY</code> 所设置的时间，来实现不规律的请求时间间隔</p>
<h4 id="User-Agent"><a href="#User-Agent" class="headerlink" title="User-Agent"></a>User-Agent</h4><p>​    <strong>用户代理，会包含在请求头部信息中</strong>，使服务端可以识别客户端，<strong>其中包括用户端的操作系统及版本，CPU类型，浏览器及版本等等一些信息</strong>。这是反爬最简单的一种设置，因为使用 Request 或是 Scrapy 等去请求页面时，请求信息中的 UA 信息不会自己表名或是使用本地浏览器的，而对于不同的浏览器，都有各自的 UA 信息</p>
<p>​    如果服务端会对 UA 进行检测，最简单的方式可以在 settings 中去设置一个 UA 的列表，然后在 yield 之前，随机从配置中获取一条 UA 并放入到字典后，给到 Request 的 headers 参数。这种方式的问题是需要自己去维护一个 UA 列表，且对于多个 spider 来说，同样的操作可能需要在多个 spider 实现</p>
<p>​    还有一种方式是直接使用 github 的随机获取 UA 的项目 fake-useragent，可以使用 pip 去安装  <code>pip intsall fake-useragent</code>，这个库提供了一个 UserAgent 类，可以实例化这个类，去调用不同的浏览器获取不同的 UA，同时还可以直接调用它的 random 方法来随机获取 UA</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fake_useragent <span class="keyword">import</span> UserAgent</span><br><span class="line">ua = UserAgent()</span><br><span class="line"></span><br><span class="line">ua.ie</span><br><span class="line"><span class="comment"># Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US);</span></span><br><span class="line">ua.msie</span><br><span class="line"><span class="comment"># Mozilla/5.0 (compatible; MSIE 10.0; Macintosh; Intel Mac OS X 10_7_3; Trident/6.0)&#x27;</span></span><br><span class="line">ua[<span class="string">&#x27;Internet Explorer&#x27;</span>]</span><br><span class="line"><span class="comment"># Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.1; Trident/4.0; GTB7.4; InfoPath.2; SV1; .NET CLR 3.3.69573; WOW64; en-US)</span></span><br><span class="line">ua.opera</span><br><span class="line"><span class="comment"># Opera/9.80 (X11; Linux i686; U; ru) Presto/2.8.131 Version/11.11</span></span><br><span class="line">ua.chrome</span><br><span class="line"><span class="comment"># Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.2 (KHTML, like Gecko) Chrome/22.0.1216.0 Safari/537.2&#x27;</span></span><br></pre></td></tr></table></figure>
<p>​    有了这个库，就省去自己去维护一个列表，并且可以很方便的去获取，但还有一点前面说到可能会有多个 spider 都会需要这个 UA，即使获取很方便了，但是这样依然还需要给每个 spider 单独操作</p>
<h5 id="下载器中间件"><a href="#下载器中间件" class="headerlink" title="下载器中间件"></a>下载器中间件</h5><p>​    请求和响应从引擎到下载器中间会经过一个下载中间件 download middleware，所以可以自己创建一个 middleware 来对 request 去进行 UA 头部信息添加的操作，这样每一个 spider 的请求在经过下载器中间时都会触发这个 middleware 来自动加上 UA，最后只需要在 settings 的 <code>DOWNLOADER_MIDDLEWARES</code> 中配置上创建的 middleware 即可</p>
<p>​    <strong>下载中间件，提供了 process_request，process_response，process_exception 三个方法</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fake_useragent <span class="keyword">import</span> UserAgent</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RandomUserAgentMiddleware</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;随机产生user-agent&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, crawler</span>):</span><br><span class="line">        <span class="comment"># 调用父类初始化</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.ua = UserAgent()</span><br><span class="line">        self.ua_type = crawler.settings.get(<span class="string">&quot;USER_AGENT_TYPE&quot;</span>, <span class="string">&quot;random&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">from_crawler</span>(<span class="params">cls, crawler</span>):</span><br><span class="line">        <span class="comment"># from_crawler 方法是 scrapy 用来创建一个爬虫的起点</span></span><br><span class="line">        s = cls(crawler)</span><br><span class="line">        <span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_request</span>(<span class="params">self, request, spider</span>):</span><br><span class="line">        <span class="comment"># 爬虫会先执行 process_request 方法在进入下载器之前</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">get_ua</span>():</span><br><span class="line">            <span class="comment"># 在函数内部定义函数，类执行 ua.ua_type 方法</span></span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">getattr</span>(self.ua, self.ua_type)</span><br><span class="line">        request.headers.setdefault(<span class="string">&quot;User-Agent&quot;</span>, get_ua())</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<p>​    使用 fake-useragent 并不绝对有用的，因为这个项目自己维护了一个 UA 信息，虽然是会更新但也可能有很多浏览器版本很老的 UA，某些网站可能会将很老版本的浏览器 UA 给禁掉，那么获取的 UA 也就无法使用了</p>
<p>​    <strong><code>process_request(request, spider)</code> 方法可以返回三个值，分别是 None ， Response 和 Request，或者抛出异常 IgnoreRequest</strong></p>
<p>​    返回 None，scrapy 将继续处理该 request，并执行其他 downloader middleware 中 process_request 方法，直到最后执行完成得到 response 才结束</p>
<p>​    返回 Response，那么 Downloader middleware 中优先级低的中间件，他们中的 process_request 和 process_exception 方法都不会被继续调用执行，然后 Downloader middleware 中每一个中间件的 process_response 方法则会被依次调用执行做各自的处理，最后将这个 response 返回给 spider</p>
<p>​    返回 Request，则这个对象会再次被放入到调度队列中，等待 scheduler 调用后，会再一次按照 Downloader middleware 中的优先级顺序去执行 process_request 方法</p>
<p>​    如果中间件中有 lgnoreRequest 被抛出，会依次执行 process_exception 方法去处理这个异常，如果没有被处理，则会触发 Request 的 errorback 方法回调，直到最后没有一个中间件处理它就会被忽略</p>
<h3 id="定制-spider-配置"><a href="#定制-spider-配置" class="headerlink" title="定制 spider 配置"></a>定制 spider 配置</h3><p>​    settings 文件的配置是应用于项目中创建的所有的 spider 的，这就可能会一个问题，对于不同的 spider 爬取的网站是不同的，比如最简单的 COOKIES_ENABLED 的配置。对于不用登陆网站，将改配置设为默认 False 是没问题，而对于需要登录的网站，如果禁止 cookie 携带，那爬虫程序就会无法去正常执行</p>
<p>​    所以，scrapy 中也提供对不同 spider 进行一些配置的私有定制， <strong>scrapy 中 spider 的 <code>__init__.py</code> 的源码中有一个属性 <code>custom_settings</code>，并且会去调用 <code>update_settings</code> 类方法将类和该属性的值更新到 settings</strong></p>
<p>​    因此，可以在一些需要定制配置的 spider 爬虫类中，创建一个  <code>custom_settings</code> 字典，将定制的配置加入进去</p>
<p>​    scrapy 的 settings 常见配置：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">BOT_NAME = <span class="string">&#x27;xxx&#x27;</span>       <span class="comment"># 配置项目名称</span></span><br><span class="line">SPIDER_MODULES = [<span class="string">&#x27;Amazon.spiders&#x27;</span>]      <span class="comment"># 配置爬虫应用的路径</span></span><br><span class="line">NEWSPIDER_MODULE = <span class="string">&#x27;Amazon.spiders&#x27;</span></span><br><span class="line">USER_AGENT = <span class="string">&#x27;xxxxxxxx&#x27;</span>     <span class="comment"># 设置请求头的用户代理信息</span></span><br><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span>      <span class="comment"># 是否遵循爬取的网站设置的爬虫协议，网站会有 robots.txt 文件固定爬取的限制</span></span><br><span class="line">COOKIES_ENABLED = <span class="literal">False</span>     <span class="comment"># 不使用 scrapy 的 cookie 中间件，开启的话请求都会 cookie 携带设置项，关闭的话，也可以 yield 的时候，自定义 headers</span></span><br><span class="line">DOWNLOAD_DELAY = <span class="number">3</span>     <span class="comment"># 设置固定的下载延迟时间</span></span><br><span class="line">RANDOMIZE_DOWNLOAD_DELAY = <span class="literal">True</span>    <span class="comment"># 设置一个 0.5 - 1.5 的随机值和 DOWNLOAD_DELAY 的值相乘，做随机延迟</span></span><br><span class="line">DEPTH_LIMIT = <span class="number">3</span>    <span class="comment"># 设置爬取的深度，从初始页面开始，跟进几次新的页面</span></span><br><span class="line">SPIDER_MIDDLEWARES = &#123;<span class="string">&#x27;中间件&#x27;</span>: 优先级&#125;    <span class="comment"># 配置 spider 的中间件，对请求和响应做处理</span></span><br><span class="line">DOWNLOADER_MIDDLEWARES = &#123;<span class="string">&#x27;中间件&#x27;</span>: 优先级&#125;    <span class="comment"># 配置下载器的中间件</span></span><br></pre></td></tr></table></figure>
<h3 id="spider-中间件"><a href="#spider-中间件" class="headerlink" title="spider 中间件"></a>spider 中间件</h3><p>​    是 scrapy 中一个重要组件，介于 engine 和 spider，用来处理 engine 发送过来的 response 对象，以及返回 spider 中产生的 item 和 request  对象，即：</p>
<p>​    1）在 Downloader 生成 response 发送给 spider 之前，对 response 做处理</p>
<p>​    2）在 spider 生成的 request 发送给 scheduler 之前，对 request 做处理</p>
<p>​    3）在 spider 生成 item 发送给 item pipeline 之前，对 item 进行处理 </p>
<p>​    当一个 spider 类被创建时，在 middleware.py 中会自动生成一个 spider 的 middleware 类，并将方法全部列举出来，大体上来说和 Downloader middleware 一样</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ArticlespiderSpiderMiddleware</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="comment"># Not all methods need to be defined. If a method is not defined,</span></span><br><span class="line">    <span class="comment"># scrapy acts as if the spider middleware does not modify the</span></span><br><span class="line">    <span class="comment"># passed objects.</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">from_crawler</span>(<span class="params">cls, crawler</span>):</span><br><span class="line">        <span class="comment"># This method is used by Scrapy to create your spiders.</span></span><br><span class="line">        s = cls()</span><br><span class="line">        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)</span><br><span class="line">        <span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_spider_input</span>(<span class="params">self, response, spider</span>):</span><br><span class="line">        <span class="comment"># Called for each response that goes through the spider</span></span><br><span class="line">        <span class="comment"># middleware and into the spider.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Should return None or raise an exception.</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_spider_output</span>(<span class="params">self, response, result, spider</span>):</span><br><span class="line">        <span class="comment"># Called with the results returned from the Spider, after</span></span><br><span class="line">        <span class="comment"># it has processed the response.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Must return an iterable of Request, dict or Item objects.</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> result:</span><br><span class="line">            <span class="keyword">yield</span> i</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_spider_exception</span>(<span class="params">self, response, exception, spider</span>):</span><br><span class="line">        <span class="comment"># Called when a spider or process_spider_input() method</span></span><br><span class="line">        <span class="comment"># (from other spider middleware) raises an exception.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Should return either None or an iterable of Request, dict</span></span><br><span class="line">        <span class="comment"># or Item objects.</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_start_requests</span>(<span class="params">self, start_requests, spider</span>):</span><br><span class="line">        <span class="comment"># Called with the start requests of the spider, and works</span></span><br><span class="line">        <span class="comment"># similarly to the process_spider_output() method, except</span></span><br><span class="line">        <span class="comment"># that it doesn’t have a response associated.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Must return only requests (not items).</span></span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> start_requests:</span><br><span class="line">            <span class="keyword">yield</span> r</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">spider_opened</span>(<span class="params">self, spider</span>):</span><br><span class="line">        spider.logger.info(<span class="string">&#x27;Spider opened: %s&#x27;</span> % spider.name)</span><br></pre></td></tr></table></figure>
<p>​    如果要启用 spider middleware，同样需要在 settings 中去进行配置 SPIDER_MIDDLEWARES，如果 spider middleware 中有不使用的，可以使用注释，也可以将 middleware 的值赋值为 None</p>
<p>​    process_spider_input 方法用来处理 response，结果要么返回 None 去继续通过其他的中间件，要么是抛出异常。process_spider_output 方法则是在处理 response 返回 result 时，会被调用，且返回结果必须是 request 或是 item 对象</p>
<h4 id="深度控制"><a href="#深度控制" class="headerlink" title="深度控制"></a>深度控制</h4><p>​    DepthMiddleware 是 scrapy 内置的一个 spider middleware，用来跟踪每个 Request 在爬取网站中的深度。DepthMiddleware 提供了一个配置 DEPTH_LIMIT，去设置深度值，注意这个深度是相对初始的 start_urls 的，如果该值为 0 表示不做深度限制，start_urls 中 url 是第一层，也就是说 DEPTH_LIMIT = 2，就只允许爬取到 start_urls 中 url 的下层 url</p>
<p>​    DEPTH_PRIORITY 配置用根据深度调整 Request 的优先级，默认是 0，即不根据深度调整</p>
<h3 id="嵌入-selenium"><a href="#嵌入-selenium" class="headerlink" title="嵌入 selenium"></a>嵌入 selenium</h3><p>​    scrapy 本身获取到的 response 中其实是不包含动态数据部分的，这也就意味着对于这部分的内容要么通过 yield 将动态数据的 url 给下载器去下载获取 json 数据，要么就是通过利用 selenium 的 webdirver 来启动一个浏览器实例来获取包含动态数据内容的 response</p>
<p>​    同样，对于可能的多个 spider 都会利用到 selenium 获取动态数据的 response，那么将 selenium 直接嵌入到 middleware 中，是最方便的一种做法。如果要另做区分，可以在中间件中通过 spider 的 name 来判断是否使用 selenium，另外，在中间件中使用 selenium 的 webdriver 请求一个页面后，就无需再将请求给下载器了，所以在中间件中就可以 return 返回 webdriver 得到的页面</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.http <span class="keyword">import</span> HtmlResponse</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">JSPageMiddleware</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;通过嵌入selenium来使用chromedriver来获取动态页面&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_request</span>(<span class="params">self, request, spider</span>):</span><br><span class="line">        <span class="comment"># process_request 处理引擎发送过来的 request</span></span><br><span class="line">        <span class="keyword">if</span> spider.name == <span class="string">&quot;blogs&quot;</span>:</span><br><span class="line">            <span class="comment"># 通过 name 判断哪个 spider 采取使用 webdriver 去获取</span></span><br><span class="line">            spider.browser.get(request.url)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">import</span> time</span><br><span class="line">            time.sleep(<span class="number">3</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;访问<span class="subst">&#123;request.url&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 但是用这个中间去获取动态页面后，那解析文件中则不需在去发起请求，所以直接返回response</span></span><br><span class="line">            <span class="keyword">return</span> HtmlResponse(url=spider.browser.current_url, body=spider.browser.page_source, encoding=<span class="string">&quot;utf8&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>​    上面的代码中并没有出现 selenium 和 webdirver 的实例，是因为如果将 webdirver 的实例过程放在 middleware 中，每个 spider 都会经过这个中间件，这就会导致，每一个请求过来都会实例化一个 webdriver 对象，这显然不合理。所以上面的 webdriver 实例这一步骤就可以放到 spider 类中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BlogsSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;blogs&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;news.blogs.com&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;https://news.cnblogs.com/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;在初始化方法中定义 webdirver&quot;&quot;&quot;</span></span><br><span class="line">        self.browser = webdriver.Chrome(executable_path=<span class="string">r&#x27;./utils/chromedriver.exe&#x27;</span>)</span><br><span class="line">        <span class="built_in">super</span>(CnblogsSpider, self).__init__()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p>​    通过直接在 spider 类的 init 中去实例化 webdirver 对象，再在 middleware 中的 process_request 方法里通过传入 spider 参数，就可以实现用实例化的浏览器对象去 get 页面</p>
<p>​     middleware 中 return 使用 HTTPResponse 来返回获取的页面，不用在 HTTPResponse 的参数中需要指明 url，body，encoding。body 直接调用 page_source 即可，而 encoding 则需要根据网页自己的编码去设置</p>
<p>​    最后，很重要的一点，就是当爬取完成后，需要关闭 webdriver。因为 webdirver 对象是在 spider 类的初始化方法里实现的，所以就可以通过 from_crawl 这个类方法来对 webdirver 的浏览器进行关闭，scrapy 中提供了信号量去对爬虫的不同状态做针对处理</p>
<p>​    <strong>from_crawler 方法的参数 crawler 代表的是 Crawler 的实例对象，Crawler 这个类是 Scrapy API 的主要入口，提供了对核心组件的访问，每一个创建 spider 类的执行都会先走 from_crawler 去创建出 spider 的对象</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BlogsSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;blogs&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;news.blogs.com&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;https://news.cnblogs.com/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;在初始化方法中定义 webdirver&quot;&quot;&quot;</span></span><br><span class="line">        self.browser = webdriver.Chrome(executable_path=<span class="string">r&#x27;./utils/chromedriver.exe&#x27;</span>)</span><br><span class="line">        <span class="built_in">super</span>(CnblogsSpider, self).__init__()</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">from_crawler</span>(<span class="params">cls, crawler, *args, **kwargs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;重写方法来实现爬虫结束后，自动将 webdirver 的浏览器关闭&quot;&quot;&quot;</span></span><br><span class="line">        spider = <span class="built_in">super</span>(BlogsSpider, cls).from_crawler(crawler, *args, **kwargs)</span><br><span class="line">        crawler.signals.connect(spider.closed, signal=signals.spider_closed)</span><br><span class="line">        <span class="keyword">return</span> spider</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">closed</span>(<span class="params">self</span>):</span><br><span class="line">        self.browser.quit()</span><br></pre></td></tr></table></figure>
<p>​    from_crawler 方法最后需要返回 spider 对象，所以可以使用 super 去继承默认创建的 spider，然后通过参数 crawler 去调用 signals.connect 方法，connect 需要接受两个参数 receiver 和 signal。receiver 是一个函数，signal 就是一个信号。当 spider 的执行结束后，会自动关闭，触发 spider_closed 的信号，然后回调自定义的 closed 方法去关闭 webdriver 的实例</p>
<h3 id="暂停与启动"><a href="#暂停与启动" class="headerlink" title="暂停与启动"></a>暂停与启动</h3><p>​    对于一些数据会做定期更细的网页来说，当 spider 执行完成一次持久化数据后，或则是 spider 执行到某一阶段需要暂停，在此之后还需要再次运行 spider 或是重新运行</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy  crawl  spider[爬虫名]  -s  JOBDIR=info/001</span><br></pre></td></tr></table></figure>
<p>​    -s 参数指定存储目录 JOBDIR，并加上用来存储当前爬取的状态信息目录（例如这里的 info 文件夹，是在项目目录下创建），对于目录设置也可以放到 settings 文件或在 <code>custom_settings</code> 属性中配置，然后在 main 的 debug 中将参数和配置加上    </p>
<p>​    上面的命令被执行后，和正常情况一样去执行，不过一旦需要暂停，需要通过按下  <code>Ctrl + C</code>来执行（如果连按下两次 <code>Ctrl + C</code> 则会强制退出，后台的进程会被强制杀死）。另外，暂停不是立刻就停下的，后台依然会做一些处理，包括调度器还没执行的 request，过滤器的数据，spider 本身的状态，这些状态的信息在暂停时需要保存</p>
<p>​    不过 <strong>scrapy 中不同的 spider 不能使用同一个目录去存储状态信息数据</strong>，所以需要另外给出一个目录。当 spider 被暂停后，给出 info/001 目录下会自动生成状态储存文件</p>
<p>​    request.seen：表示已经访问的 url</p>
<p>​    request.state：表示 spider 的状态信息</p>
<p>​    requests.queue：是一个文件夹，包含两个文件，active.json 和 p0；active.json：是一个序列化后的 json 文件；p0：表示需要继续完成的 request，当再次运行时且全部运行完成，这个文件就会被删除</p>
<p>​    对于暂停来说，还需要注意一点，就是网站的 cookie 信息，这个是有时效性的，如果在一段时间内没有恢复，可能在再次恢复启动时，就会遇到问题</p>
<h3 id="Telnet-服务"><a href="#Telnet-服务" class="headerlink" title="Telnet 服务"></a>Telnet 服务</h3><p>​    简单来说，就是利用 telent 来实现远程操作，也就是说可将整个 scrapy 放到服务器上，通过本地使用 telent 去操控 scrapy 的 spider。前提是本地需要通过控制面板，去安装 telent 这个功能服务</p>
<p>​    spider 在运行时，其实默认会去监听 telent 的服务，每次执行命令行中都会输出的这个信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023</span><br></pre></td></tr></table></figure>
<p>​    从提示信息可知，默认会在本地的 6023 端口去监听 telent 服务，这时可以在命令行在开启一个窗口，输入 <code>telent localhost 6023</code> 就可连接到 scrapy</p>
<p>​    监听的接口，是由 TELNETCONSOLE_HOST 配置决定，默认值就是 127.0.0.1；监听的端口则是由 </p>
<p>TELNETCONSOLE_PORT 控制，默认是一个范围 [6023,6073]，当配置为 None 或 0，则是动态分配一个端口</p>
<p>​    且在 telent 的终端上，输入   <code>est()</code> 就会返回一系列的 scrapy 的状态信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">telnet localhost 6023</span><br><span class="line">&gt;&gt;&gt; est()</span><br><span class="line">Execution engine status</span><br><span class="line"></span><br><span class="line"># 启动的时长</span><br><span class="line">time()-engine.start_time                        : 8.62972998619</span><br><span class="line">engine.has_capacity()                           : False</span><br><span class="line">len(engine.downloader.active)                   : 16</span><br><span class="line">engine.scraper.is_idle()                        : False</span><br><span class="line">engine.spider.name                              : followall</span><br><span class="line">engine.spider_is_idle(engine.spider)            : False</span><br><span class="line">engine.slot.closing                             : False</span><br><span class="line">len(engine.slot.inprogress)                     : 16</span><br><span class="line">len(engine.slot.scheduler.dqs or [])            : 0</span><br><span class="line">len(engine.slot.scheduler.mqs)                  : 92</span><br><span class="line">len(engine.scraper.slot.queue)                  : 0</span><br><span class="line">len(engine.scraper.slot.active)                 : 0</span><br><span class="line">engine.scraper.slot.active_size                 : 0</span><br><span class="line">engine.scraper.slot.itemproc_size               : 0</span><br><span class="line">engine.scraper.slot.needs_backout()             : False</span><br></pre></td></tr></table></figure>
<pre><code>除此之外，Telnet 终端也提供了一些变量，来方便查看一些信息</code></pre><table>
<thead>
<tr>
<th>快捷名称</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td><code>crawler</code></td>
<td>Scrapy Crawler (<code>scrapy.crawler.Crawler</code>对象)</td>
</tr>
<tr>
<td><code>engine</code></td>
<td>Crawler.engine属性</td>
</tr>
<tr>
<td><code>spider</code></td>
<td>当前激活的爬虫(spider)</td>
</tr>
<tr>
<td><code>slot</code></td>
<td>the engine slot</td>
</tr>
<tr>
<td><code>extensions</code></td>
<td>扩展管理器(manager) (Crawler.extensions属性)</td>
</tr>
<tr>
<td><code>stats</code></td>
<td>状态收集器 (Crawler.stats属性)</td>
</tr>
<tr>
<td><code>settings</code></td>
<td>Scrapy设置(setting)对象 (Crawler.settings属性)</td>
</tr>
<tr>
<td><code>est</code></td>
<td>打印引擎状态的报告</td>
</tr>
<tr>
<td><code>prefs</code></td>
<td>针对内存调试</td>
</tr>
<tr>
<td><code>p</code></td>
<td><a target="_blank" rel="noopener" href="http://docs.python.org/library/pprint.html#pprint.pprint">pprint.pprint</a> 函数的简写</td>
</tr>
<tr>
<td><code>hpy</code></td>
<td>针对内存调试</td>
</tr>
</tbody></table>
<p>​    其中，通过 engine 这个变量可以实现远程操作 scrapy 的暂停，恢复以及停止</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 控制引擎暂停</span><br><span class="line">engine.pause()</span><br><span class="line"># 恢复</span><br><span class="line">engine.unpause()</span><br><span class="line"># 停止</span><br><span class="line">engine.stop()</span><br></pre></td></tr></table></figure>
<h3 id="状态数据收集（stats-collection）"><a href="#状态数据收集（stats-collection）" class="headerlink" title="状态数据收集（stats collection）"></a>状态数据收集（stats collection）</h3><p>​    <strong>scrapy 提供的状态收集机制都是以 key-value 存储的</strong> 计数型的，通过 crawl 类去调用 stats 来使用，且 stats collection 无论是开启还是关闭都是可以使用的</p>
<p>​    设置一个数据，使用     <code>stats.set_value(&#39;xxx&#39;,xxx())</code>；如果需要每次累加，则是 <code>stats.inc_value(&#39;xxx&#39;)</code>；想要获取数据，使用 <code>stats.get_value(&#39;xxx&#39;)</code>；如果是要获取所有的数据，使用 <code>stats.get_stats()</code></p>
<p>​    当要设置一个比原来值大的时候，<code>stats.max_value(&#39;max_items_scraped&#39;, value)</code>，反之则是 <code>stats.min_value(&#39;min_free_memory_percent&#39;, value)</code></p>
<h3 id="分布式爬取"><a href="#分布式爬取" class="headerlink" title="分布式爬取"></a>分布式爬取</h3><p>​    分布式爬取的目的就是为了将单独的爬虫程序部署在多台服务器上，但是单个的 scrapy 爬虫如果要部署到多台服务器上，就会面临一些问题。比如，多个爬虫之间怎么知道当前的 url 是否爬取过，这就涉及到多个爬虫如何获取 url，以及在分布式下如何做到 url 的去重</p>
<p>​    对于分布式来说，最常见的一种方式，通过设置一个队列，让多台服务器去对其进行监听，当队列中有数据时，监听的调度器会自动获取一个数据，然后开始处理。而对于 url 的去重，本质上 scrapy 也是将 url 放到 set 实现的，但在分布式情况下，就需要一个统一容器来存放多台服务器爬取过的 url，实现上虽然都是 set ，但是从原本的数据结构的方式，变为了数据库存储的形式</p>
<p>​    对于这个队列的实现，scrapy-redis 的 queue 中包含了三个基本的队列：FIFO 队列，LIFO 队列，Priority 队类，三者都通过继承 Base 基类去实现，其中基类定义基本的接口方法，要求子类去实现</p>
<p>​    <strong>Base 基类中还有两个又来序列化和反序列化的方法：_encode_request 和 _decode_request</strong>，前者用来对 Request 对象进行序列化，这样才能保存到数据库上，后者就是读取保存的序列化对象进行反序列化得到原本的 Request 对象，而序列化和反序列化的方式是利用 pickle 模块去实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Base</span>(<span class="title class_ inherited__">object</span>): </span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Per-spider base queue class&quot;&quot;&quot;</span> </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, server, spider, key, serializer=<span class="literal">None</span></span>): </span><br><span class="line">        <span class="keyword">if</span> serializer <span class="keyword">is</span> <span class="literal">None</span>: </span><br><span class="line">            serializer = picklecompat </span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(serializer, <span class="string">&#x27;loads&#x27;</span>): </span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">&quot;serializer does not implement &#x27;loads&#x27; function: % r&quot;</span> % serializer) </span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(serializer, <span class="string">&#x27;dumps&#x27;</span>): </span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">&quot;serializer &#x27;% s&#x27; does not implement &#x27;dumps&#x27; function: % r&quot;</span> % serializer) </span><br><span class="line">        self.server = server </span><br><span class="line">        self.spider = spider </span><br><span class="line">        self.key = key % &#123;<span class="string">&#x27;spider&#x27;</span>: spider.name&#125; </span><br><span class="line">        self.serializer = serializer </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_encode_request</span>(<span class="params">self, request</span>): </span><br><span class="line">        obj = request_to_dict(request, self.spider) </span><br><span class="line">        <span class="keyword">return</span> self.serializer.dumps(obj) </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_decode_request</span>(<span class="params">self, encoded_request</span>): </span><br><span class="line">        obj = self.serializer.loads(encoded_request) </span><br><span class="line">        <span class="keyword">return</span> request_from_dict(obj, self.spider) </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>): </span><br><span class="line">        <span class="string">&quot;&quot;&quot;Return the length of the queue&quot;&quot;&quot;</span> </span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">push</span>(<span class="params">self, request</span>): </span><br><span class="line">        <span class="string">&quot;&quot;&quot;Push a request&quot;&quot;&quot;</span> </span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">pop</span>(<span class="params">self, timeout=<span class="number">0</span></span>): </span><br><span class="line">        <span class="string">&quot;&quot;&quot;Pop a request&quot;&quot;&quot;</span> </span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">clear</span>(<span class="params">self</span>): </span><br><span class="line">        <span class="string">&quot;&quot;&quot;Clear queue/stack&quot;&quot;&quot;</span> </span><br><span class="line">        self.server.delete(self.key) </span><br></pre></td></tr></table></figure>
<p>​    利用分布式爬取的特点也很明显，在爬取效率和速度上，多台服务器的带宽可以大大提高爬取的速度；同时利用多台服务器爬取，也减少了 ip 地址被检测爬虫的概率</p>
<p>​    对于 scrapy 框架来说，有一个专门用于分布式爬取的项目 <em>scrapy-redis</em>，它将 requests 的状态管理以及 url 的去重全部放在 redis 上实现</p>
<p>​    <strong>使用步骤：</strong></p>
<p>​    1）确保环境中安装了 redis，可以简单点直接使用   <code>pip install redis</code></p>
<p>​    2）从 github 上 clone 项目： <code>git clone https://github.com/rmax/scrapy-redis.git</code>，也可以直接下载压缩包</p>
<p>​    3）将 clone 的 scrapy-redis 项目下 src 目录下的 scrapy_redis 文件夹复制到爬虫项目中</p>
<p>​    5） 按照 scrapy-redis 项目要求，修改爬虫项目的 settings 中的 SCHEDULER，DUPEFILTER_CLASS 以及 ITEM_PIPELINES，ITEM_PIPELINES 的目的是为了将 item 存储在 redis 上，这样可以实现一个共享，且因为数据都在 item 中，如果要实现持久化，可以直接从 redis 中获取去做数据入库</p>
<p>​     对于爬虫项目的编写，使用 scrapy_redis 和不使用都是一样的，不同之处是 scrapy_redis 中 spider 继承了 scrapy_redis.spiders.RedisSpider 这个类，同时这个类还继承了 RedisMixin 和原本 scrapy 的 Spider 类，所以本身 spider 的编写也就是用的 scrapy 的</p>
<p>​    其中，对于每一个 spider 类，都会额外有一个属性叫做 <strong>redis_key</strong>，默认格式是 spider 名加上初始的 start_urls，中间使用冒号隔开</p>
<p>​    scrapy_redis 会在 redis 服务中创建两个 key，分别是：    <code>myspider:requests</code> 和 <code>myspider:dupefilter</code>。前者是一个有序集合 zset，因为 scrapy 支持对爬取设置优先级，而 redis 可以通过有序集合设置分数来实现排序，同时监听 redis 的 spider 会依次从里面去取序列化的 Request，后者就是普通的无序字典 set，用来存储爬取过的 url 的 fingerprint</p>
<p>​    <strong>运行步骤：</strong></p>
<p>​    <strong>1）使用 <code>scrapy runspier myspider.py</code> 来运行爬虫文件，启动后 scrapy 会对本地的 redis 进行监听，分布式下，这个 redis 可以在任意一个服务器上</strong></p>
<p>​    <strong>2）使用 <code>redis-cli lpush myspider:start_urls https://xxxxxx.com</code>，通过 lpush 将初始 url 将入到 redis 队列中，让正在监听的 spider 去获取</strong></p>
<h3 id="增量式爬虫"><a href="#增量式爬虫" class="headerlink" title="增量式爬虫"></a>增量式爬虫</h3><p>​    增量式爬虫与一般爬虫程序本质上没有太大区别，但是增量式的关注点不只是进行爬取，而是判别数据是否已经爬过，从而判断内容是否更新以至于需要再次爬取</p>
<p>​    同时 <strong>增量式爬取也可能涉及两种不同状态：一种是全量的爬虫程序还没有停止，二是如果爬虫程序关闭</strong></p>
<p>​    对于第一种情况，可以在全量爬虫进行时，另外开启一个服务启动爬虫程序来做为增量爬取，或者是利用 scrapy-redis，但是需要对这个源码进行改动。对于正在执行的全量爬虫来说，一旦网站出现了新的数据都会将 Request 发出来，scrapy-redis 原本都会将其加入队列中，所以可以使用优先级队列，来给新增的 Request 的参数中加上优先级，这样自动会去优先对新出现的内容去进行爬取</p>
<p>​    使用 scrapy-redis 的优先级队列，需要在 settings 文件中配置  <code>SCHEDULER_QUEUE_CLASS = &quot;scrapy_redis.queue.PriorityQueue&quot;</code></p>
<p>​    对于第二种情况，爬虫程序结束了的话，毫无疑问需要再次启动，但是为了方便管理，可以通过第三方组件或是脚本程序来设置定时启动，因为对于网站的内容而言，发布过的数据内容更新频率不会太高（但是这依然需要根据网站的设计去单独讨论，这样的方式不能适用于所有的网站）。还有一种情况就是半关闭（暂停或等待）状态，这种情况也非常好处理，因为 scrapy-redis 默认要从队列里取请求，等待状态下，只需要将 url 通过 push 加入 redis 中即可</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">Legacy</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/1649325139/">http://example.com/1649325139/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank"></a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/scrapy/">scrapy</a></div><div class="post_share"><div class="social-share" data-image="https://wei-foun.github.io/img/cover25.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/3927654560/"><img class="prev-cover" src="https://wei-foun.github.io/img/cover26.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Mysql-整理-二</div></div></a></div><div class="next-post pull-right"><a href="/3797262603/"><img class="next-cover" src="https://wei-foun.github.io/img/cover23.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">计算机网络</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/162388192/" title="Cookie池"><img class="cover" src="https://wei-foun.github.io/img/cover28.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-01</div><div class="title">Cookie池</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Legacy</div><div class="author-info__description">冒险的生涯在召唤！</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">49</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">31</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">46</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Live a life you will remember</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Scrapy-%E6%A1%86%E6%9E%B6"><span class="toc-number">1.</span> <span class="toc-text">Scrapy 框架</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA-scrapy-%E9%A1%B9%E7%9B%AE"><span class="toc-number">1.1.</span> <span class="toc-text">创建一个 scrapy 项目</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%88%AC%E8%99%AB"><span class="toc-number">1.2.</span> <span class="toc-text">创建一个爬虫</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8%E4%B8%80%E4%B8%AA%E7%88%AC%E8%99%AB%E7%A8%8B%E5%BA%8F"><span class="toc-number">1.3.</span> <span class="toc-text">启动一个爬虫程序</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9E%B6%E6%9E%84%E7%BB%84%E4%BB%B6"><span class="toc-number">1.4.</span> <span class="toc-text">架构组件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80%E4%B8%AA-spider-%E6%95%B0%E6%8D%AE%E6%B5%81%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B"><span class="toc-number">1.5.</span> <span class="toc-text">一个 spider 数据流执行过程</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8F%90%E5%8F%96%E6%96%B9%E5%BC%8F"><span class="toc-number">2.</span> <span class="toc-text">提取方式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Xpath"><span class="toc-number">2.1.</span> <span class="toc-text">Xpath</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#CSS-%E9%80%89%E6%8B%A9%E5%99%A8"><span class="toc-number">2.2.</span> <span class="toc-text">CSS 选择器</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B7%9F%E8%BF%9B%E7%88%AC%E5%8F%96"><span class="toc-number">3.</span> <span class="toc-text">跟进爬取</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E8%B0%83%E8%AF%95"><span class="toc-number">4.</span> <span class="toc-text">命令行的调试</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#item-%E6%95%B0%E6%8D%AE%E4%BC%A0%E9%80%92"><span class="toc-number">5.</span> <span class="toc-text">item 数据传递</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BE%E7%89%87%E8%87%AA%E5%8A%A8%E4%B8%8B%E8%BD%BD"><span class="toc-number">6.</span> <span class="toc-text">图片自动下载</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Json-%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8"><span class="toc-number">7.</span> <span class="toc-text">Json 文件存储</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%8C%81%E4%B9%85%E5%8C%96"><span class="toc-number">8.</span> <span class="toc-text">数据持久化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BB%E9%94%AE%E5%86%B2%E7%AA%81"><span class="toc-number">8.1.</span> <span class="toc-text">主键冲突</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%82%E6%AD%A5%E5%85%A5%E5%BA%93"><span class="toc-number">8.2.</span> <span class="toc-text">异步入库</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#item-loaders"><span class="toc-number">9.</span> <span class="toc-text">item loaders</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Field-%E7%9A%84%E5%AE%9A%E5%88%B6"><span class="toc-number">9.1.</span> <span class="toc-text">Field 的定制</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#scrapy-%E7%9A%84-url-%E5%8E%BB%E9%87%8D"><span class="toc-number">10.</span> <span class="toc-text">scrapy 的 url 去重</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8E%BB%E9%87%8D%E7%AD%96%E7%95%A5"><span class="toc-number">10.1.</span> <span class="toc-text">去重策略</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CrawlSpider"><span class="toc-number">11.</span> <span class="toc-text">CrawlSpider</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA-CrawlSpider"><span class="toc-number">11.1.</span> <span class="toc-text">创建一个 CrawlSpider</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8F%90%E5%8F%96%E8%A7%84%E5%88%99"><span class="toc-number">11.2.</span> <span class="toc-text">提取规则</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%8D%E7%88%AC%E6%9C%BA%E5%88%B6%E4%B8%8E%E5%8F%8D%E7%88%AC%E7%AD%96%E7%95%A5"><span class="toc-number">12.</span> <span class="toc-text">反爬机制与反爬策略</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A2%91%E7%8E%87"><span class="toc-number">12.1.</span> <span class="toc-text">频率</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#User-Agent"><span class="toc-number">12.2.</span> <span class="toc-text">User-Agent</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%8B%E8%BD%BD%E5%99%A8%E4%B8%AD%E9%97%B4%E4%BB%B6"><span class="toc-number">12.2.1.</span> <span class="toc-text">下载器中间件</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E5%88%B6-spider-%E9%85%8D%E7%BD%AE"><span class="toc-number">13.</span> <span class="toc-text">定制 spider 配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#spider-%E4%B8%AD%E9%97%B4%E4%BB%B6"><span class="toc-number">14.</span> <span class="toc-text">spider 中间件</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E6%8E%A7%E5%88%B6"><span class="toc-number">14.1.</span> <span class="toc-text">深度控制</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B5%8C%E5%85%A5-selenium"><span class="toc-number">15.</span> <span class="toc-text">嵌入 selenium</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9A%82%E5%81%9C%E4%B8%8E%E5%90%AF%E5%8A%A8"><span class="toc-number">16.</span> <span class="toc-text">暂停与启动</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Telnet-%E6%9C%8D%E5%8A%A1"><span class="toc-number">17.</span> <span class="toc-text">Telnet 服务</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%8A%B6%E6%80%81%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%EF%BC%88stats-collection%EF%BC%89"><span class="toc-number">18.</span> <span class="toc-text">状态数据收集（stats collection）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E5%8F%96"><span class="toc-number">19.</span> <span class="toc-text">分布式爬取</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A2%9E%E9%87%8F%E5%BC%8F%E7%88%AC%E8%99%AB"><span class="toc-number">20.</span> <span class="toc-text">增量式爬虫</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/466764255/" title="airflow整理"><img src="https://wei-foun.github.io/img/cover47.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="airflow整理"/></a><div class="content"><a class="title" href="/466764255/" title="airflow整理">airflow整理</a><time datetime="2025-07-12T07:04:30.000Z" title="发表于 2025-07-12 15:04:30">2025-07-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2133446919/" title="Linux再学习"><img src="https://wei-foun.github.io/img/cover46.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Linux再学习"/></a><div class="content"><a class="title" href="/2133446919/" title="Linux再学习">Linux再学习</a><time datetime="2025-07-12T06:58:04.000Z" title="发表于 2025-07-12 14:58:04">2025-07-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/4075015966/" title="GO 基础"><img src="https://wei-foun.github.io/img/cover45.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="GO 基础"/></a><div class="content"><a class="title" href="/4075015966/" title="GO 基础">GO 基础</a><time datetime="2025-07-04T16:26:58.000Z" title="发表于 2025-07-05 00:26:58">2025-07-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/4209932858/" title="docker 容器网络"><img src="https://wei-foun.github.io/img/cover44.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="docker 容器网络"/></a><div class="content"><a class="title" href="/4209932858/" title="docker 容器网络">docker 容器网络</a><time datetime="2023-07-04T16:26:58.000Z" title="发表于 2023-07-05 00:26:58">2023-07-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/4253636491/" title="kubernetes-搭建"><img src="https://wei-foun.github.io/img/cover43.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="kubernetes-搭建"/></a><div class="content"><a class="title" href="/4253636491/" title="kubernetes-搭建">kubernetes-搭建</a><time datetime="2023-03-19T10:39:02.000Z" title="发表于 2023-03-19 18:39:02">2023-03-19</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://wei-foun.github.io/img/cover25.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Legacy</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'MHzSjOElX9Cf5IJAfoNr4COL-gzGzoHsz',
      appKey: 'K3d5HK6zRMD2BINwstEANt7H',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: {"tv_doge":"6ea59c827c414b4a2955fe79e0f6fd3dcd515e24.png","tv_親親":"a8111ad55953ef5e3be3327ef94eb4a39d535d06.png","tv_偷笑":"bb690d4107620f1c15cff29509db529a73aee261.png","tv_再見":"180129b8ea851044ce71caf55cc8ce44bd4a4fc8.png","tv_冷漠":"b9cbc755c2b3ee43be07ca13de84e5b699a3f101.png","tv_發怒":"34ba3cd204d5b05fec70ce08fa9fa0dd612409ff.png","tv_發財":"34db290afd2963723c6eb3c4560667db7253a21a.png","tv_可愛":"9e55fd9b500ac4b96613539f1ce2f9499e314ed9.png","tv_吐血":"09dd16a7aa59b77baa1155d47484409624470c77.png","tv_呆":"fe1179ebaa191569b0d31cecafe7a2cd1c951c9d.png","tv_嘔吐":"9f996894a39e282ccf5e66856af49483f81870f3.png","tv_困":"241ee304e44c0af029adceb294399391e4737ef2.png","tv_壞笑":"1f0b87f731a671079842116e0991c91c2c88645a.png","tv_大佬":"093c1e2c490161aca397afc45573c877cdead616.png","tv_大哭":"23269aeb35f99daee28dda129676f6e9ea87934f.png","tv_委屈":"d04dba7b5465779e9755d2ab6f0a897b9b33bb77.png","tv_害羞":"a37683fb5642fa3ddfc7f4e5525fd13e42a2bdb1.png","tv_尷尬":"7cfa62dafc59798a3d3fb262d421eeeff166cfa4.png","tv_微笑":"70dc5c7b56f93eb61bddba11e28fb1d18fddcd4c.png","tv_思考":"90cf159733e558137ed20aa04d09964436f618a1.png","tv_驚嚇":"0d15c7e2ee58e935adc6a7193ee042388adc22af.png"},
      path: window.location.pathname,
      visitor: true
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>